{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle submission notebook  \n",
    "\n",
    "This notebook is to make submission on Kaggle based on the research and experiments performed in the main notebook.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:13.044624Z",
     "iopub.status.busy": "2025-06-20T10:23:13.044297Z",
     "iopub.status.idle": "2025-06-20T10:23:16.830539Z",
     "shell.execute_reply": "2025-06-20T10:23:16.829367Z",
     "shell.execute_reply.started": "2025-06-20T10:23:13.044601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:16.833811Z",
     "iopub.status.busy": "2025-06-20T10:23:16.832661Z",
     "iopub.status.idle": "2025-06-20T10:23:16.840635Z",
     "shell.execute_reply": "2025-06-20T10:23:16.839607Z",
     "shell.execute_reply.started": "2025-06-20T10:23:16.833777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:16.841803Z",
     "iopub.status.busy": "2025-06-20T10:23:16.841561Z",
     "iopub.status.idle": "2025-06-20T10:23:17.316898Z",
     "shell.execute_reply": "2025-06-20T10:23:17.316022Z",
     "shell.execute_reply.started": "2025-06-20T10:23:16.841781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (17307, 3)\n",
      "Test Data Shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "\n",
    "print(\"Train Data Shape:\", train_df.shape)\n",
    "print(\"Test Data Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:17.318929Z",
     "iopub.status.busy": "2025-06-20T10:23:17.318653Z",
     "iopub.status.idle": "2025-06-20T10:23:17.325723Z",
     "shell.execute_reply": "2025-06-20T10:23:17.324773Z",
     "shell.execute_reply.started": "2025-06-20T10:23:17.318901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  essay_id                                          full_text\n",
      "0  000d118  Many people have car where they live. The thin...\n",
      "1  000fe60  I am a scientist at NASA that is discussing th...\n",
      "2  001ab80  People always wish they had the same technolog...\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:17.326984Z",
     "iopub.status.busy": "2025-06-20T10:23:17.326680Z",
     "iopub.status.idle": "2025-06-20T10:23:58.244292Z",
     "shell.execute_reply": "2025-06-20T10:23:58.243445Z",
     "shell.execute_reply.started": "2025-06-20T10:23:17.326957Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Text Data: 100%|██████████| 17307/17307 [00:40<00:00, 423.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 s, sys: 871 ms, total: 40.9 s\n",
      "Wall time: 40.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean the text data by:\n",
    "        - Lowercasing the text\n",
    "        - Removing URLs \n",
    "        - Removing special characters and punctuation\n",
    "        - Tokenizing the text\n",
    "        - Removing stopwords\n",
    "        - Lemmatizing the tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning Text Data\")\n",
    "train_df['cleaned_text'] = train_df['full_text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:58.245552Z",
     "iopub.status.busy": "2025-06-20T10:23:58.245219Z",
     "iopub.status.idle": "2025-06-20T10:24:00.801014Z",
     "shell.execute_reply": "2025-06-20T10:24:00.800186Z",
     "shell.execute_reply.started": "2025-06-20T10:23:58.245525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vectorize_text(train, column='cleaned_text', num_features=1000):\n",
    "    \"\"\"\n",
    "    To apply TF-IDF vectorization to the text data.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=num_features)\n",
    "    X_train_vec = vectorizer.fit_transform(train[column])\n",
    "    return X_train_vec,vectorizer\n",
    "\n",
    "set_of_tf_idf_features = {}\n",
    "\n",
    "num_features=3000\n",
    "X_train_vec,  vectorizer = vectorize_text(train_df, num_features=num_features)\n",
    "set_of_tf_idf_features[3000] = {\n",
    "    'X_train': X_train_vec,\n",
    "    'vectorizer': vectorizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:24:00.802132Z",
     "iopub.status.busy": "2025-06-20T10:24:00.801874Z",
     "iopub.status.idle": "2025-06-20T10:24:31.902312Z",
     "shell.execute_reply": "2025-06-20T10:24:31.901466Z",
     "shell.execute_reply.started": "2025-06-20T10:24:00.802113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 131 ms, total: 1min 1s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# best model LightGBM Regressor\tTF-IDF 3000 features\n",
    "test_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "test_df['cleaned_text'] = test_df['full_text'].apply(clean_text)\n",
    "X_test_vec = set_of_tf_idf_features[3000]['vectorizer'].transform(test_df['cleaned_text'])\n",
    "\n",
    "# retraining the best model on the full training set\n",
    "parameters = {'num_leaves': 12\n",
    "              , 'max_depth': -1\n",
    "              , 'learning_rate': 0.1\n",
    "              , 'n_estimators': 500}\n",
    "final_model = LGBMRegressor(**parameters\n",
    "                            , class_weight='balanced'\n",
    "                            , objective='regression'\n",
    "                            , random_state=42\n",
    "                            , verbose=-1)\n",
    "\n",
    "final_model.fit(X_train_vec, train_df['score'])\n",
    "\n",
    "# Making predictions on the test set\n",
    "test_predictions = final_model.predict(X_test_vec)\n",
    "test_predictions = np.round(test_predictions).astype(int)\n",
    "test_predictions = np.clip(test_predictions, 1, 6)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'essay_id': test_df['essay_id'],\n",
    "    'score': test_predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

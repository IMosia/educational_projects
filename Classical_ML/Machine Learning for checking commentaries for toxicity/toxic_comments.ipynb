{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments toxicity for Wikishop\n",
    "Online store \"Wikishop\" launches a new service. Now users can edit and supplement product descriptions like in Wiki communities. That is, customers offer their edits and comments on the changes of others. The store needs a tool that will search for toxic comments and send them for moderation.\n",
    "\n",
    "We need to train the model to categorize comments into positive and negative. For this purpose, we have at our disposal a dataset with markup on the toxicity of edits.\n",
    "\n",
    "Customer requirement: a model with a quality metric value *F1* of at least 0.75.\n",
    "\n",
    "**Project execution**\n",
    "\n",
    "1. Download and prepare the data.\n",
    "2. Train the different models. \n",
    "3. Draw conclusions.\n",
    "\n",
    "**Description of data**\n",
    "\n",
    "The data is in the file `toxic_comments.csv`. The *text* column in it contains the text of the comment, and *toxic* contains the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Familiarizing-yourself-with-the-data.\" data-toc-modified-id=\"Familiarizing-yourself-with-the-data.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Familiarizing yourself with the data.</a></span></li><li><span><a href=\"#Cleaing-comment's-text-universal-for-TF-IDF\" data-toc-modified-id=\"Cleaing-comment's-text-universal-for-TF-IDF-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Cleaing comment's text universal for TF-IDF</a></span></li><li><span><a href=\"#Class-disbalance\" data-toc-modified-id=\"Class-disbalance-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Class disbalance</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Auxiliary-Functions\" data-toc-modified-id=\"Auxiliary-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Auxiliary Functions</a></span></li><li><span><a href=\"#Dummy-model\" data-toc-modified-id=\"Dummy-model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Dummy model</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Decision-tree\" data-toc-modified-id=\"Decision-tree-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Decision tree</a></span></li><li><span><a href=\"#Random-forest\" data-toc-modified-id=\"Random-forest-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Random forest</a></span></li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>LightGBM</a></span></li><li><span><a href=\"#Passive-Aggressive-Algorithm\" data-toc-modified-id=\"Passive-Aggressive-Algorithm-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Passive-Aggressive Algorithm</a></span></li><li><span><a href=\"#Models-Comparison\" data-toc-modified-id=\"Models-Comparison-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Models Comparison</a></span></li><li><span><a href=\"#Validation-on-test-sample-and-additional-metrics\" data-toc-modified-id=\"Validation-on-test-sample-and-additional-metrics-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Validation on test sample and additional metrics</a></span></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, precision_recall_curve, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Familiarizing yourself with the data.\n",
    "Consider the data:\n",
    "The dataset contains 159292 comments, three columns: comment, toxicity sign, and an unnamed column, most likely wrongly saved old indexes, it can be removed. No duplicates or missing values were found.\n",
    "The text needs to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159292, 3) \n",
      "\n",
      "Index(['Unnamed: 0', 'text', 'toxic'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "text          0\n",
       "toxic         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31015</th>\n",
       "      <td>31055</td>\n",
       "      <td>Sometime back, I just happened to log on to ww...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102832</th>\n",
       "      <td>102929</td>\n",
       "      <td>\"\\n\\nThe latest edit is much better, don't mak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67317</th>\n",
       "      <td>67385</td>\n",
       "      <td>\" October 2007 (UTC)\\n\\nI would think you'd be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81091</th>\n",
       "      <td>81167</td>\n",
       "      <td>Thanks for the tip on the currency translation...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90091</th>\n",
       "      <td>90182</td>\n",
       "      <td>I would argue that if content on the Con in co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>1860</td>\n",
       "      <td>\"=Reliable sources===\\nCheating:\\n\"\"Barry Bond...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125293</th>\n",
       "      <td>125422</td>\n",
       "      <td>WTF=\\n\\nHow The Fuck Does This Person Merit A ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148986</th>\n",
       "      <td>149142</td>\n",
       "      <td>cajuns, acadians\\nCajuns, acadians, louisianan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89697</th>\n",
       "      <td>89784</td>\n",
       "      <td>Hi - I dropped a pin in Google Maps at the cer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64256</th>\n",
       "      <td>64323</td>\n",
       "      <td>Re removal of accessdate= for urls books \\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  toxic\n",
       "31015        31055  Sometime back, I just happened to log on to ww...      0\n",
       "102832      102929  \"\\n\\nThe latest edit is much better, don't mak...      0\n",
       "67317        67385  \" October 2007 (UTC)\\n\\nI would think you'd be...      0\n",
       "81091        81167  Thanks for the tip on the currency translation...      0\n",
       "90091        90182  I would argue that if content on the Con in co...      0\n",
       "1860          1860  \"=Reliable sources===\\nCheating:\\n\"\"Barry Bond...      1\n",
       "125293      125422  WTF=\\n\\nHow The Fuck Does This Person Merit A ...      1\n",
       "148986      149142  cajuns, acadians\\nCajuns, acadians, louisianan...      0\n",
       "89697        89784  Hi - I dropped a pin in Google Maps at the cer...      0\n",
       "64256        64323  Re removal of accessdate= for urls books \\n\\nT...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17217</th>\n",
       "      <td>Murrumbidgee means 'track goes down here', 'a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic\n",
       "17217  Murrumbidgee means 'track goes down here', 'a ...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('C:/Users/Ivan/datasetsYP/toxic_comments.csv')\n",
    "\n",
    "print(df.shape, '\\n')\n",
    "print(df.columns)\n",
    "display(df.info())\n",
    "\n",
    "display(df.isna().sum())\n",
    "print(df.duplicated().sum())\n",
    "display(df.sample(10))\n",
    "\n",
    "\n",
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "display(df.sample(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaing comment's text universal for TF-IDF\n",
    "We remove all non-english alphabetic and symbol spaces, work only in low register, then lemmatization will be performed utilizing WordnetLemmatizer and additional self-made function for determination of part of speech (the last one slows the algorithm significantly, but it is necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86a69a994e54e44ad1132b19f346985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  The    striped bats are hanging on_their-feet,...\n",
      "1  you should    be !*? ashamedываыва ofя yoursel...\n",
      "0    the strip bat be hang on their foot for best\n",
      "1       you should be ashamed of yourself go work\n",
      "Name: text, dtype: object\n",
      "CPU times: total: 1h 9min 37s\n",
      "Wall time: 1h 15min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_and_clean(text):\n",
    "    low_text = text.lower()\n",
    "    eng_text = re.sub(r'[^a-z ]', ' ', low_text)\n",
    "    words = eng_text.split()\n",
    "    lemm_words = [wnl.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    lemm_text = ' '.join(lemm_words)\n",
    "    return lemm_text\n",
    "    \n",
    "df['corpus'] = df['text'].progress_apply(lemmatize_and_clean)\n",
    "\n",
    "# example to check that lemmatization and cleaning works correctly\n",
    "sentence1 = \"The    striped bats are hanging on_their-feet,.,/ for best\"\n",
    "sentence2 = \"you should    be !*? ashamedываыва ofя yourself went worked\"\n",
    "df_my = pd.DataFrame([sentence1, sentence2], columns = ['text'])\n",
    "print(df_my)\n",
    "print(df_my['text'].apply(lemmatize_and_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the prepared data TfidfVectorizer will be applied, and resulting parameters dataset will be splited into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62904     hi aoystreck thanks for contribute to help mak...\n",
       "127078    image npi jpg list for deletion an image or me...\n",
       "41890     if you read the reoprt there be two reaction i...\n",
       "79625     merge with substance abuse addiction and drug ...\n",
       "99312             a jerusalem be consider the mother church\n",
       "154191    i fear i may have misrepresent the rd edition ...\n",
       "96381     thank you for the explanation i will in future...\n",
       "156534    request for third opinion user chubbles and my...\n",
       "64915     it be in unbiased faith which might turn out b...\n",
       "29447     september please do not add nonsense to wikipe...\n",
       "Name: corpus, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (127433, 132732)\n",
      "Shape of X_test: (31859, 132732)\n",
      "Shape of y_test: (31859,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<31859x132732 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 821509 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stopwords = list(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "X_train_not_transformed, X_test_not_transformed, y_train, y_test = train_test_split(\n",
    "                                                                                    df['corpus'], df['toxic'],\n",
    "                                                                                    test_size=0.2,\n",
    "                                                                                    stratify=df['toxic']\n",
    "                                                                                    )\n",
    "display(X_train_not_transformed.sample(10))\n",
    "\n",
    "X_train = count_tf_idf.fit_transform(X_train_not_transformed)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "X_test = count_tf_idf.transform(X_test_not_transformed)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "display(X_test)\n",
    "dense_matrix = X_test.todense()\n",
    "display(dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class disbalance\n",
    "Since we are learning a classification task, and not every second comment on the internet is offensive, we need to check the data for balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic comments to all comments: 10.161213369158526 %\n"
     ]
    }
   ],
   "source": [
    "print('Toxic comments to all comments:', \n",
    "     (df['toxic'].sum() / len(df)) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% is quite small, so prediction by the mean will have an accuracy of 90%.\n",
    "Since there is a lot of data, and training models on it will be difficult, we will use downsample method for balancing.\n",
    "In addition, we will train models with built-in class balancing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before transformation: 127433\n",
      "Old toxic to all comments ratio: 10.161418156992303 %\n",
      "До:\n",
      " 0    114484\n",
      "1     12949\n",
      "Name: toxic, dtype: int64\n",
      "Data after transformation: 25898\n",
      "Ammount of data after transfirmation to before: 20.322836313984602\n",
      "New toxic to all comments ration: 50.0 %\n",
      "After:\n",
      " 0    12949\n",
      "1    12949\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler()\n",
    "\n",
    "print(f'Data before transformation: {X_train.shape[0]}')\n",
    "print('Old toxic to all comments ratio:', \n",
    "     (y_train.sum() / len(y_train)) * 100, '%')\n",
    "print('До:\\n', y_train.value_counts())\n",
    "\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f'Data after transformation: {X_train_resampled.shape[0]}')\n",
    "print(f'Ammount of data after transfirmation to before: {100*X_train_resampled.shape[0]/X_train.shape[0]}')\n",
    "print('New toxic to all comments ration:', \n",
    "     (y_train_resampled.sum() / len(y_train_resampled)) * 100, '%')\n",
    "print('After:\\n',y_train_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = pd.DataFrame({\n",
    "                    'model_name': [],\n",
    "                    'balance': [],\n",
    "                    'f1': [],\n",
    "                    'comments': [],\n",
    "                    'model': [],\n",
    "                    'learning_time': []\n",
    "})\n",
    "\n",
    "def append_model_list(model_name, balance, f1_score, comment, best_model, learning_time):\n",
    "    \"\"\"\n",
    "    Function to add model and prediction information to the global list\n",
    "    \"\"\"\n",
    "    global model_list\n",
    "    new_row = {'model_name': model_name, 'f1': f1_score, 'balance': balance,\n",
    "               'comments': comment, 'model': best_model, 'learning_time': learning_time}\n",
    "    model_list = model_list.append(new_row, ignore_index=True)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def train_and_predict(which_model, model_name, balance, parameters, features, targets):\n",
    "    \"\"\"\n",
    "    Function for training models, making predictions and fitting hyperparameters\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    model = GridSearchCV(which_model, param_grid=parameters,\n",
    "                             scoring='f1', cv=kf)\n",
    "    \n",
    "    model.fit(features, targets)\n",
    "    cv_results = cross_val_score(model, features, targets, cv=kf, scoring='f1')\n",
    "    \n",
    "    print(f'Model {model_name} c {balance}')\n",
    "    print(f'cv_results = {cv_results.mean():.2f} +- {cv_results.std():.2f}')\n",
    "    print(model.best_params_)\n",
    "    \n",
    "    end = time.time()\n",
    "    learning_time = end-start\n",
    "    \n",
    "    print('Learning time:', learning_time)\n",
    "    \n",
    "    comment = model.best_params_\n",
    "    append_model_list(model_name, balance, cv_results.mean(), comment, model, learning_time)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_me_all_metrics(name, true, predictions, probability, model):\n",
    "    \"\"\"\n",
    "    Function for calculating various parameters and visualization\n",
    "    for better model calculations\n",
    "    Shows: confusion matrix, Precision-Recall curve, ROC curve \n",
    "    Calculates: precision, recall, f1, accuracy, roc_auc_score\n",
    "    Transmitted data:\n",
    "    name - model name\n",
    "    true - true values\n",
    "    predictions - model predictions\n",
    "    probability - model probabilities\n",
    "    \"\"\"\n",
    "    print(f'Analysis for {name}')\n",
    "    \n",
    "    confusion_m = confusion_matrix(\n",
    "        true,\n",
    "        predictions,\n",
    "        labels=None,\n",
    "        sample_weight=None,\n",
    "        normalize=None,\n",
    "    )\n",
    "    display(confusion_m)\n",
    "    \n",
    "    pr = precision_score(true, predictions)\n",
    "    print(f'Precision: {round(pr, 3)}')\n",
    "\n",
    "    re = recall_score(true, predictions)\n",
    "    print(f'Recall: {round(re, 3)}')\n",
    "\n",
    "    f1 = f1_score(true, predictions)\n",
    "    print(f'F1: {round(f1, 3)}')\n",
    "    \n",
    "    accuracy = accuracy_score(true, predictions)\n",
    "    print(f'accuracy: {round(accuracy, 3)}')\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        true,\n",
    "        probability[:,1],\n",
    "        pos_label=None,\n",
    "        sample_weight=None,\n",
    "    )\n",
    "    \n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall curve\")\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(true, probability[:, 1])\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    roc_auc_score_val = roc_auc_score(true, probability[:, 1])\n",
    "    print(f'roc_auc_score: {round(roc_auc_score_val, 3)}')\n",
    "    \n",
    "    if not ('Dummy' in name):\n",
    "        try:\n",
    "            print(model.best_estimator_.get_feature_names_out().tolist())\n",
    "        except:\n",
    "            print(model.best_estimator_.get_params())\n",
    "        finally:\n",
    "            pass\n",
    "    return \n",
    "\n",
    "\n",
    "def predict_and_metrics_on_test(model_name, model, X_test = X_test, y_test = y_test):\n",
    "    \"\"\"\n",
    "    Function for the final prediction on the test sample and calling the full visualization for it\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    probability = model.predict_proba(X_test)\n",
    "    show_me_all_metrics(model_name, y_test, predictions, probability, model)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy model\n",
    "As we have lower amount of toxic comments, dummy prediction will be all toxic.\n",
    "Otherwise, with no class balance dummy model will win over anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Dummy 1 c Downsample\n",
      "cv_results = 0.53 +- 0.45\n",
      "{}\n",
      "Learning time: 0.2908921241760254\n",
      "Model Dummy 1 c Self\n",
      "cv_results = 0.18 +- 0.00\n",
      "{}\n",
      "Learning time: 1.2591304779052734\n",
      "Model Dummy 1 c No\n",
      "cv_results = 0.18 +- 0.00\n",
      "{}\n",
      "Learning time: 1.07289457321167\n"
     ]
    }
   ],
   "source": [
    "param_dummy = {}\n",
    "\n",
    "train_and_predict(DummyClassifier(strategy = 'constant', constant = 1), 'Dummy 1', 'Downsample',\n",
    "                  param_dummy, X_train_resampled, y_train_resampled)\n",
    "\n",
    "train_and_predict(DummyClassifier(strategy = 'constant', constant = 1), 'Dummy 1', 'Self',\n",
    "                  param_dummy, X_train, y_train)\n",
    "\n",
    "train_and_predict(DummyClassifier(strategy = 'constant', constant = 1), 'Dummy 1', 'No',\n",
    "                  param_dummy, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression c Downsample\n",
      "cv_results = 0.54 +- 0.44\n",
      "{'C': 10}\n",
      "Learning time: 212.8493525981903\n",
      "Model LogisticRegression c Self\n",
      "cv_results = 0.76 +- 0.01\n",
      "{'C': 10}\n",
      "Learning time: 332.4433026313782\n",
      "Model LogisticRegression c No\n",
      "cv_results = 0.77 +- 0.01\n",
      "{'C': 10}\n",
      "Learning time: 319.6209132671356\n",
      "CPU times: total: 1h 47min 54s\n",
      "Wall time: 14min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_log = {\n",
    "    'C': [10, 100, 1000],\n",
    "}\n",
    "\n",
    "train_and_predict(LogisticRegression(), 'LogisticRegression', 'Downsample',\n",
    "                  param_log, X_train_resampled, y_train_resampled)\n",
    "\n",
    "train_and_predict(LogisticRegression(class_weight=\"balanced\"), 'LogisticRegression', 'Self',\n",
    "                  param_log, X_train, y_train)\n",
    "\n",
    "train_and_predict(LogisticRegression(), 'LogisticRegression', 'No',\n",
    "                  param_log, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model DecisionTree c Downsample\n",
      "cv_results = 0.43 +- 0.35\n",
      "{'criterion': 'gini', 'max_depth': 20}\n",
      "Learning time: 159.22210574150085\n",
      "\n",
      "\n",
      "Model DecisionTree c Self\n",
      "cv_results = 0.62 +- 0.01\n",
      "{'criterion': 'gini', 'max_depth': 20}\n",
      "Learning time: 1657.5758242607117\n",
      "\n",
      "\n",
      "Model DecisionTree c No\n",
      "cv_results = 0.66 +- 0.01\n",
      "{'criterion': 'gini', 'max_depth': 20}\n",
      "Learning time: 1420.7902572154999\n",
      "CPU times: total: 53min 46s\n",
      "Wall time: 53min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_tree = {\n",
    "    'max_depth': [2, 10, 20],\n",
    "    'criterion': ['gini'],\n",
    "}\n",
    "\n",
    "train_and_predict(DecisionTreeClassifier(), 'DecisionTree', 'Downsample',\n",
    "                  param_tree, X_train_resampled, y_train_resampled)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(DecisionTreeClassifier(class_weight=\"balanced\"), 'DecisionTree', 'Self',\n",
    "                  param_tree, X_train, y_train)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(DecisionTreeClassifier(), 'DecisionTree', 'No',\n",
    "                  param_tree, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RandomForest c Downsample\n",
      "cv_results = 0.18 +- 0.28\n",
      "{'max_depth': 10, 'n_estimators': 10}\n",
      "Learning time: 156.16220450401306\n",
      "\n",
      "\n",
      "Model RandomForest c Self\n",
      "cv_results = 0.35 +- 0.00\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "Learning time: 1253.0160298347473\n",
      "\n",
      "\n",
      "Model RandomForest c No\n",
      "cv_results = 0.00 +- 0.01\n",
      "{'max_depth': 10, 'n_estimators': 10}\n",
      "Learning time: 1069.1295776367188\n",
      "CPU times: total: 41min 7s\n",
      "Wall time: 41min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_forest = {\n",
    "    'n_estimators': [10, 100],\n",
    "    'max_depth': [2, 10],\n",
    "}\n",
    "\n",
    "train_and_predict(RandomForestClassifier(), 'RandomForest', 'Downsample',\n",
    "                  param_forest, X_train_resampled, y_train_resampled)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(RandomForestClassifier(class_weight=\"balanced\"), 'RandomForest', 'Self',\n",
    "                  param_forest, X_train, y_train)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(RandomForestClassifier(), 'RandomForest', 'No',\n",
    "                  param_forest, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107784\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2791\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107795\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2773\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114163\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2911\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499952 -> initscore=-0.000193\n",
      "[LightGBM] [Info] Start training from score -0.000193\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120274\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3031\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120387\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3036\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107784\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2791\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107795\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2773\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 114163\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2911\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499952 -> initscore=-0.000193\n",
      "[LightGBM] [Info] Start training from score -0.000193\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120274\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3031\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120387\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3036\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107784\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2791\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107795\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2773\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114163\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2911\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499952 -> initscore=-0.000193\n",
      "[LightGBM] [Info] Start training from score -0.000193\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120274\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3031\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120387\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3036\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107784\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2791\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107795\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2773\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114163\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2911\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499952 -> initscore=-0.000193\n",
      "[LightGBM] [Info] Start training from score -0.000193\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120274\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3031\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120387\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3036\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 139970\n",
      "[LightGBM] [Info] Number of data points in the train set: 25898, number of used features: 3388\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80443\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81323\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2261\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90261\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91306\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2479\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90782\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2481\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80443\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 81323\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2261\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90261\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91306\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2479\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90782\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2481\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80443\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81323\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2261\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90261\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91306\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2479\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90782\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2481\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 80443\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81323\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2261\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90261\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91306\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2479\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90782\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2481\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107784\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2791\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 79990\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2224\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 81892\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2276\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90559\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2459\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91464\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90864\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2475\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 79990\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2224\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81892\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2276\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90559\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2459\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91464\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90864\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2475\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 79990\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2224\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81892\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2276\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90559\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2459\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91464\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90864\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2475\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 3625\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 79990\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2224\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.781284 -> initscore=1.273164\n",
      "[LightGBM] [Info] Start training from score 1.273164\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12430, number of negative: 4144\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81892\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2276\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.749970 -> initscore=1.098451\n",
      "[LightGBM] [Info] Start training from score 1.098451\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8805, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90559\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2459\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531254 -> initscore=0.125178\n",
      "[LightGBM] [Info] Start training from score 0.125178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91464\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8806, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90864\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2475\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531282 -> initscore=0.125292\n",
      "[LightGBM] [Info] Start training from score 0.125292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 7769\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107795\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2773\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625012 -> initscore=0.510877\n",
      "[LightGBM] [Info] Start training from score 0.510877\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 86749\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2391\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87462\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Info] Number of positive: 8286, number of negative: 8288\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91501\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2469\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499940 -> initscore=-0.000241\n",
      "[LightGBM] [Info] Start training from score -0.000241\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97952\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2607\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 97258\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2591\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 86749\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2391\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87462\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Info] Number of positive: 8286, number of negative: 8288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91501\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2469\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499940 -> initscore=-0.000241\n",
      "[LightGBM] [Info] Start training from score -0.000241\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97952\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2607\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97258\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2591\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 86749\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2391\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87462\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8286, number of negative: 8288\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91501\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2469\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499940 -> initscore=-0.000241\n",
      "[LightGBM] [Info] Start training from score -0.000241\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97952\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2607\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97258\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2591\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 86749\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2391\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 6216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87462\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624955 -> initscore=0.510633\n",
      "[LightGBM] [Info] Start training from score 0.510633\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8286, number of negative: 8288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91501\n",
      "[LightGBM] [Info] Number of data points in the train set: 16574, number of used features: 2469\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499940 -> initscore=-0.000241\n",
      "[LightGBM] [Info] Start training from score -0.000241\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 97952\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2607\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 6215, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97258\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2591\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374962 -> initscore=-0.510987\n",
      "[LightGBM] [Info] Start training from score -0.510987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10358, number of negative: 10360\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 114163\n",
      "[LightGBM] [Info] Number of data points in the train set: 20718, number of used features: 2911\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499952 -> initscore=-0.000193\n",
      "[LightGBM] [Info] Start training from score -0.000193\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93138\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2503\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94128\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 93144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2493\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 102504\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2680\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 103612\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2706\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93138\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2503\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94128\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2493\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 102504\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2680\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 103612\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2706\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 93138\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2503\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 94128\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2493\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 102504\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2680\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 103612\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2706\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93138\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2503\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94128\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2493\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 102504\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2680\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 103612\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2706\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120274\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3031\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 93546\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2540\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 94144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2538\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93429\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2521\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 102309\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2683\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 103979\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2713\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 93546\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2540\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2538\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93429\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2521\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 102309\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2683\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 103979\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2713\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 93546\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2540\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 94144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2538\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 93429\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2521\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 102309\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2683\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 103979\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2713\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93546\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2540\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94144\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2538\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 8805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 93429\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2521\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468778 -> initscore=-0.125050\n",
      "[LightGBM] [Info] Start training from score -0.125050\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4143, number of negative: 12432\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 102309\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575, number of used features: 2683\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.249955 -> initscore=-1.098854\n",
      "[LightGBM] [Info] Start training from score -1.098854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3627, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 103979\n",
      "[LightGBM] [Info] Number of data points in the train set: 16576, number of used features: 2713\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218810 -> initscore=-1.272613\n",
      "[LightGBM] [Info] Start training from score -1.272613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7770, number of negative: 12949\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120387\n",
      "[LightGBM] [Info] Number of data points in the train set: 20719, number of used features: 3036\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375018 -> initscore=-0.510748\n",
      "[LightGBM] [Info] Start training from score -0.510748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Model LGBMClassifier c Downsample\n",
      "cv_results = 0.50 +- 0.41\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "Learning time: 198.97772693634033\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.334495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.321315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.319692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.320185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.352060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.317912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.365092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.330558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.336304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.328988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.396789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.358477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.383119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.419790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.339768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.330096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.323737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.337028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.308553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.410822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 114484\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.012897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 548026\n",
      "[LightGBM] [Info] Number of data points in the train set: 127433, number of used features: 9929\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.908411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.055045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.970823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.990304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.986675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.034898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.983980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.977139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.987612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.955784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.033515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.995955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.982370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.086947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.974745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.974342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.978502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.998647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.040212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.972452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.390114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.980014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.936640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.061135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.981188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.981956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.980363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.984529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.970241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.053833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.977393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.968129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.034221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.991064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.998193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.026063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.922538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.882258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.059302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.996854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.788328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.204631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.967646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.902506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.958743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.938989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.989294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.929242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.934401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.955315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.919655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.023516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.952588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.939317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.965347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.958130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.951581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.934451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.929773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.916565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.020033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.938449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.340294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.937976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.932194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.021350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.918966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.921637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.933221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.020108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.948419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.963389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.943388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.939397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.030863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.975005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.945068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.034611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.958502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.959422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.954751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.933121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.935161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.383539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.011860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.934968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.940463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.944996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.007423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.935583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.926612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.016810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.935377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.932986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.923213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.895845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.932422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.016017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.938546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.949926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.942824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.940582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.022399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.934056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.359341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Model LGBMClassifier c Self\n",
      "cv_results = 0.73 +- 0.00\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "Learning time: 956.1908118724823\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.327828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101171 -> initscore=-2.184278\n",
      "[LightGBM] [Info] Start training from score -2.184278\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.329847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101936 -> initscore=-2.175893\n",
      "[LightGBM] [Info] Start training from score -2.175893\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.416452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101318 -> initscore=-2.182661\n",
      "[LightGBM] [Info] Start training from score -2.182661\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.361348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101827 -> initscore=-2.177083\n",
      "[LightGBM] [Info] Start training from score -2.177083\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.360346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101818 -> initscore=-2.177190\n",
      "[LightGBM] [Info] Start training from score -2.177190\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.390619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101171 -> initscore=-2.184278\n",
      "[LightGBM] [Info] Start training from score -2.184278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.333940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101936 -> initscore=-2.175893\n",
      "[LightGBM] [Info] Start training from score -2.175893\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.386807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101318 -> initscore=-2.182661\n",
      "[LightGBM] [Info] Start training from score -2.182661\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.339190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101827 -> initscore=-2.177083\n",
      "[LightGBM] [Info] Start training from score -2.177083\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.287998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101818 -> initscore=-2.177190\n",
      "[LightGBM] [Info] Start training from score -2.177190\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.289978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101171 -> initscore=-2.184278\n",
      "[LightGBM] [Info] Start training from score -2.184278\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.449251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101936 -> initscore=-2.175893\n",
      "[LightGBM] [Info] Start training from score -2.175893\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.337358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101318 -> initscore=-2.182661\n",
      "[LightGBM] [Info] Start training from score -2.182661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.323487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101827 -> initscore=-2.177083\n",
      "[LightGBM] [Info] Start training from score -2.177083\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.433711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101818 -> initscore=-2.177190\n",
      "[LightGBM] [Info] Start training from score -2.177190\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.322030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101171 -> initscore=-2.184278\n",
      "[LightGBM] [Info] Start training from score -2.184278\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.295063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101936 -> initscore=-2.175893\n",
      "[LightGBM] [Info] Start training from score -2.175893\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.335900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101318 -> initscore=-2.182661\n",
      "[LightGBM] [Info] Start training from score -2.182661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.319868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101827 -> initscore=-2.177083\n",
      "[LightGBM] [Info] Start training from score -2.177083\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.288643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101818 -> initscore=-2.177190\n",
      "[LightGBM] [Info] Start training from score -2.177190\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12949, number of negative: 114484\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.864340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 548026\n",
      "[LightGBM] [Info] Number of data points in the train set: 127433, number of used features: 9929\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101614 -> initscore=-2.179417\n",
      "[LightGBM] [Info] Start training from score -2.179417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.949358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101820 -> initscore=-2.177168\n",
      "[LightGBM] [Info] Start training from score -2.177168\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.921171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100519 -> initscore=-2.191475\n",
      "[LightGBM] [Info] Start training from score -2.191475\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.943189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.940814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101193 -> initscore=-2.184038\n",
      "[LightGBM] [Info] Start training from score -2.184038\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.999184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101316 -> initscore=-2.182691\n",
      "[LightGBM] [Info] Start training from score -2.182691\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.935661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101820 -> initscore=-2.177168\n",
      "[LightGBM] [Info] Start training from score -2.177168\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.937941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100519 -> initscore=-2.191475\n",
      "[LightGBM] [Info] Start training from score -2.191475\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.946613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.945520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101193 -> initscore=-2.184038\n",
      "[LightGBM] [Info] Start training from score -2.184038\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.010806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101316 -> initscore=-2.182691\n",
      "[LightGBM] [Info] Start training from score -2.182691\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.927860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101820 -> initscore=-2.177168\n",
      "[LightGBM] [Info] Start training from score -2.177168\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.935530 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100519 -> initscore=-2.191475\n",
      "[LightGBM] [Info] Start training from score -2.191475\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.973855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.926534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101193 -> initscore=-2.184038\n",
      "[LightGBM] [Info] Start training from score -2.184038\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.924727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101316 -> initscore=-2.182691\n",
      "[LightGBM] [Info] Start training from score -2.182691\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8304, number of negative: 73252\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.938381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397005\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101820 -> initscore=-2.177168\n",
      "[LightGBM] [Info] Start training from score -2.177168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8198, number of negative: 73359\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.935872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397126\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100519 -> initscore=-2.191475\n",
      "[LightGBM] [Info] Start training from score -2.191475\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.931929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397243\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7575\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8253, number of negative: 73304\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.980380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397964\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101193 -> initscore=-2.184038\n",
      "[LightGBM] [Info] Start training from score -2.184038\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8263, number of negative: 73294\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.932218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396700\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7562\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101316 -> initscore=-2.182691\n",
      "[LightGBM] [Info] Start training from score -2.182691\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10314, number of negative: 91632\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.336224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467308\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8672\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101171 -> initscore=-2.184278\n",
      "[LightGBM] [Info] Start training from score -2.184278\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.934983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101059 -> initscore=-2.185509\n",
      "[LightGBM] [Info] Start training from score -2.185509\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.688610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102235 -> initscore=-2.172632\n",
      "[LightGBM] [Info] Start training from score -2.172632\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.005225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101965 -> initscore=-2.175574\n",
      "[LightGBM] [Info] Start training from score -2.175574\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.117742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102149 -> initscore=-2.173567\n",
      "[LightGBM] [Info] Start training from score -2.173567\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.929287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102272 -> initscore=-2.172231\n",
      "[LightGBM] [Info] Start training from score -2.172231\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.941910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101059 -> initscore=-2.185509\n",
      "[LightGBM] [Info] Start training from score -2.185509\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.982532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102235 -> initscore=-2.172632\n",
      "[LightGBM] [Info] Start training from score -2.172632\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.955142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101965 -> initscore=-2.175574\n",
      "[LightGBM] [Info] Start training from score -2.175574\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.012716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102149 -> initscore=-2.173567\n",
      "[LightGBM] [Info] Start training from score -2.173567\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.984334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102272 -> initscore=-2.172231\n",
      "[LightGBM] [Info] Start training from score -2.172231\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.984587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101059 -> initscore=-2.185509\n",
      "[LightGBM] [Info] Start training from score -2.185509\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.042092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102235 -> initscore=-2.172632\n",
      "[LightGBM] [Info] Start training from score -2.172632\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.999300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101965 -> initscore=-2.175574\n",
      "[LightGBM] [Info] Start training from score -2.175574\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.951171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102149 -> initscore=-2.173567\n",
      "[LightGBM] [Info] Start training from score -2.173567\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.070815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102272 -> initscore=-2.172231\n",
      "[LightGBM] [Info] Start training from score -2.172231\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8242, number of negative: 73314\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.954988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396043\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101059 -> initscore=-2.185509\n",
      "[LightGBM] [Info] Start training from score -2.185509\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8338, number of negative: 73219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.042469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396477\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102235 -> initscore=-2.172632\n",
      "[LightGBM] [Info] Start training from score -2.172632\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8316, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.971303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396024\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101965 -> initscore=-2.175574\n",
      "[LightGBM] [Info] Start training from score -2.175574\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8331, number of negative: 73226\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.978390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396432\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102149 -> initscore=-2.173567\n",
      "[LightGBM] [Info] Start training from score -2.173567\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8341, number of negative: 73216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.968637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395387\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7509\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102272 -> initscore=-2.172231\n",
      "[LightGBM] [Info] Start training from score -2.172231\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10392, number of negative: 91554\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.393779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 466134\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101936 -> initscore=-2.175893\n",
      "[LightGBM] [Info] Start training from score -2.175893\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.018762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100287 -> initscore=-2.194041\n",
      "[LightGBM] [Info] Start training from score -2.194041\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.966832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102419 -> initscore=-2.170629\n",
      "[LightGBM] [Info] Start training from score -2.170629\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.996243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.997672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101377 -> initscore=-2.182018\n",
      "[LightGBM] [Info] Start training from score -2.182018\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.016832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101500 -> initscore=-2.180673\n",
      "[LightGBM] [Info] Start training from score -2.180673\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.942902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100287 -> initscore=-2.194041\n",
      "[LightGBM] [Info] Start training from score -2.194041\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.989387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102419 -> initscore=-2.170629\n",
      "[LightGBM] [Info] Start training from score -2.170629\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.014158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.992739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101377 -> initscore=-2.182018\n",
      "[LightGBM] [Info] Start training from score -2.182018\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.047694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101500 -> initscore=-2.180673\n",
      "[LightGBM] [Info] Start training from score -2.180673\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.979851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100287 -> initscore=-2.194041\n",
      "[LightGBM] [Info] Start training from score -2.194041\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.982401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102419 -> initscore=-2.170629\n",
      "[LightGBM] [Info] Start training from score -2.170629\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.043455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.983655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101377 -> initscore=-2.182018\n",
      "[LightGBM] [Info] Start training from score -2.182018\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.973195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101500 -> initscore=-2.180673\n",
      "[LightGBM] [Info] Start training from score -2.180673\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8179, number of negative: 73377\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.981807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397347\n",
      "[LightGBM] [Info] Number of data points in the train set: 81556, number of used features: 7574\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100287 -> initscore=-2.194041\n",
      "[LightGBM] [Info] Start training from score -2.194041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8353, number of negative: 73204\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.997077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397088\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7580\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102419 -> initscore=-2.170629\n",
      "[LightGBM] [Info] Start training from score -2.170629\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8238, number of negative: 73319\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.964934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396643\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101009 -> initscore=-2.186062\n",
      "[LightGBM] [Info] Start training from score -2.186062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8268, number of negative: 73289\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.045274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 398067\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7617\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101377 -> initscore=-2.182018\n",
      "[LightGBM] [Info] Start training from score -2.182018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8278, number of negative: 73279\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.037650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396876\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7563\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101500 -> initscore=-2.180673\n",
      "[LightGBM] [Info] Start training from score -2.180673\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10329, number of negative: 91617\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.419805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467710\n",
      "[LightGBM] [Info] Number of data points in the train set: 101946, number of used features: 8701\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101318 -> initscore=-2.182661\n",
      "[LightGBM] [Info] Start training from score -2.182661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.010309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100923 -> initscore=-2.187008\n",
      "[LightGBM] [Info] Start training from score -2.187008\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.984718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103057 -> initscore=-2.163713\n",
      "[LightGBM] [Info] Start training from score -2.163713\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.965161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101412 -> initscore=-2.181628\n",
      "[LightGBM] [Info] Start training from score -2.181628\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.973421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101609 -> initscore=-2.179477\n",
      "[LightGBM] [Info] Start training from score -2.179477\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.967399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102136 -> initscore=-2.173714\n",
      "[LightGBM] [Info] Start training from score -2.173714\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.686122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100923 -> initscore=-2.187008\n",
      "[LightGBM] [Info] Start training from score -2.187008\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.976022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103057 -> initscore=-2.163713\n",
      "[LightGBM] [Info] Start training from score -2.163713\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.048527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101412 -> initscore=-2.181628\n",
      "[LightGBM] [Info] Start training from score -2.181628\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.962195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101609 -> initscore=-2.179477\n",
      "[LightGBM] [Info] Start training from score -2.179477\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.953923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102136 -> initscore=-2.173714\n",
      "[LightGBM] [Info] Start training from score -2.173714\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.745866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100923 -> initscore=-2.187008\n",
      "[LightGBM] [Info] Start training from score -2.187008\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.803446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103057 -> initscore=-2.163713\n",
      "[LightGBM] [Info] Start training from score -2.163713\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.778297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101412 -> initscore=-2.181628\n",
      "[LightGBM] [Info] Start training from score -2.181628\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.872875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101609 -> initscore=-2.179477\n",
      "[LightGBM] [Info] Start training from score -2.179477\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.846186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102136 -> initscore=-2.173714\n",
      "[LightGBM] [Info] Start training from score -2.173714\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8231, number of negative: 73326\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.885608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396743\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7568\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100923 -> initscore=-2.187008\n",
      "[LightGBM] [Info] Start training from score -2.187008\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8405, number of negative: 73152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.927502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396328\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103057 -> initscore=-2.163713\n",
      "[LightGBM] [Info] Start training from score -2.163713\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8271, number of negative: 73287\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.022144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396957\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101412 -> initscore=-2.181628\n",
      "[LightGBM] [Info] Start training from score -2.181628\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8287, number of negative: 73271\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.943801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397290\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7589\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101609 -> initscore=-2.179477\n",
      "[LightGBM] [Info] Start training from score -2.179477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8330, number of negative: 73228\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.939439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396270\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7548\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102136 -> initscore=-2.173714\n",
      "[LightGBM] [Info] Start training from score -2.173714\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10381, number of negative: 91566\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.333300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 467080\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8681\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101827 -> initscore=-2.177083\n",
      "[LightGBM] [Info] Start training from score -2.177083\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.944677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100911 -> initscore=-2.187143\n",
      "[LightGBM] [Info] Start training from score -2.187143\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.946734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103044 -> initscore=-2.163845\n",
      "[LightGBM] [Info] Start training from score -2.163845\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.926578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101400 -> initscore=-2.181762\n",
      "[LightGBM] [Info] Start training from score -2.181762\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.004982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101977 -> initscore=-2.175454\n",
      "[LightGBM] [Info] Start training from score -2.175454\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.937335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101756 -> initscore=-2.177866\n",
      "[LightGBM] [Info] Start training from score -2.177866\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.943961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100911 -> initscore=-2.187143\n",
      "[LightGBM] [Info] Start training from score -2.187143\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.931956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103044 -> initscore=-2.163845\n",
      "[LightGBM] [Info] Start training from score -2.163845\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.929544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101400 -> initscore=-2.181762\n",
      "[LightGBM] [Info] Start training from score -2.181762\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.949035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101977 -> initscore=-2.175454\n",
      "[LightGBM] [Info] Start training from score -2.175454\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.945831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101756 -> initscore=-2.177866\n",
      "[LightGBM] [Info] Start training from score -2.177866\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.922565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100911 -> initscore=-2.187143\n",
      "[LightGBM] [Info] Start training from score -2.187143\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.951241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103044 -> initscore=-2.163845\n",
      "[LightGBM] [Info] Start training from score -2.163845\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.971842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101400 -> initscore=-2.181762\n",
      "[LightGBM] [Info] Start training from score -2.181762\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.940643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101977 -> initscore=-2.175454\n",
      "[LightGBM] [Info] Start training from score -2.175454\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.916972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101756 -> initscore=-2.177866\n",
      "[LightGBM] [Info] Start training from score -2.177866\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8230, number of negative: 73327\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.043324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397142\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7583\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100911 -> initscore=-2.187143\n",
      "[LightGBM] [Info] Start training from score -2.187143\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8404, number of negative: 73153\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.939625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396677\n",
      "[LightGBM] [Info] Number of data points in the train set: 81557, number of used features: 7565\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103044 -> initscore=-2.163845\n",
      "[LightGBM] [Info] Start training from score -2.163845\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8270, number of negative: 73288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.952885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396949\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7545\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101400 -> initscore=-2.181762\n",
      "[LightGBM] [Info] Start training from score -2.181762\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8317, number of negative: 73241\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.936222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 396973\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101977 -> initscore=-2.175454\n",
      "[LightGBM] [Info] Start training from score -2.175454\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 8299, number of negative: 73259\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.021122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396991\n",
      "[LightGBM] [Info] Number of data points in the train set: 81558, number of used features: 7559\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101756 -> initscore=-2.177866\n",
      "[LightGBM] [Info] Start training from score -2.177866\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10380, number of negative: 91567\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.330741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 467345\n",
      "[LightGBM] [Info] Number of data points in the train set: 101947, number of used features: 8677\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101818 -> initscore=-2.177190\n",
      "[LightGBM] [Info] Start training from score -2.177190\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Model LGBMClassifier c No\n",
      "cv_results = 0.69 +- 0.00\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "Learning time: 948.4877440929413\n",
      "CPU times: total: 4h 1min 39s\n",
      "Wall time: 35min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_LGBM = {\n",
    "    'max_depth': [2, 10],\n",
    "    'n_estimators': [10, 100]\n",
    "}\n",
    "\n",
    "train_and_predict(LGBMClassifier(), 'LGBMClassifier', 'Downsample',\n",
    "                  param_LGBM, X_train_resampled, y_train_resampled)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(LGBMClassifier(class_weight=\"balanced\"), 'LGBMClassifier', 'Self',\n",
    "                  param_LGBM, X_train, y_train)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(LGBMClassifier(), 'LGBMClassifier', 'No',\n",
    "                  param_LGBM, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive-Aggressive Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model PassiveAggressiveClassifier c Downsample\n",
      "cv_results = 0.54 +- 0.45\n",
      "{'max_iter': 2}\n",
      "Learning time: 10.448578357696533\n",
      "\n",
      "\n",
      "Model PassiveAggressiveClassifier c Self\n",
      "cv_results = 0.72 +- 0.01\n",
      "{'max_iter': 10}\n",
      "Learning time: 67.90528655052185\n",
      "\n",
      "\n",
      "Model PassiveAggressiveClassifier c No\n",
      "cv_results = 0.77 +- 0.01\n",
      "{'max_iter': 2}\n",
      "Learning time: 56.84765028953552\n",
      "CPU times: total: 10min 21s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The orgininal algorithm does not have a predict_probabilities method,\n",
    "#it is a new object that inherits everything, and has this function\n",
    "\n",
    "class PA_classifier_prob(PassiveAggressiveClassifier):\n",
    "    def _init_(self, X, y):\n",
    "        super()._init_(X, y)\n",
    "    def predict_proba(self, X):\n",
    "        #method to wrap values from decision function in a sigmoid function to get probabilities\n",
    "        arr1 = 1 - (1. / (1. + np.exp(-self.decision_function(X)))) \n",
    "        arr2 = 1 - arr1\n",
    "        return np.stack((arr2, arr1), axis=1)\n",
    "\n",
    "param_PAA = {\n",
    "    'max_iter': [2, 5, 10, 25, 50],\n",
    "}\n",
    "\n",
    "train_and_predict(PA_classifier_prob(), 'PassiveAggressiveClassifier', 'Downsample',\n",
    "                  param_PAA, X_train_resampled, y_train_resampled)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(PA_classifier_prob(class_weight=\"balanced\"), 'PassiveAggressiveClassifier', 'Self',\n",
    "                  param_PAA, X_train, y_train)\n",
    "print('\\n')\n",
    "\n",
    "train_and_predict(PA_classifier_prob(), 'PassiveAggressiveClassifier', 'No',\n",
    "                  param_PAA, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>balance</th>\n",
       "      <th>f1</th>\n",
       "      <th>comments</th>\n",
       "      <th>model</th>\n",
       "      <th>learning_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>No</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 10}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>1069.129578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Downsample</td>\n",
       "      <td>0.180954</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 10}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>156.162205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dummy 1</td>\n",
       "      <td>Self</td>\n",
       "      <td>0.184480</td>\n",
       "      <td>{}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>1.259130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dummy 1</td>\n",
       "      <td>No</td>\n",
       "      <td>0.184480</td>\n",
       "      <td>{}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>1.072895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Self</td>\n",
       "      <td>0.346738</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>1253.016030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>Downsample</td>\n",
       "      <td>0.427891</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>159.222106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>Downsample</td>\n",
       "      <td>0.497031</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>198.977727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy 1</td>\n",
       "      <td>Downsample</td>\n",
       "      <td>0.533368</td>\n",
       "      <td>{}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>0.290892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>Downsample</td>\n",
       "      <td>0.537301</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>212.849353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>Downsample</td>\n",
       "      <td>0.544872</td>\n",
       "      <td>{'max_iter': 2}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>10.448578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>Self</td>\n",
       "      <td>0.616789</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>1657.575824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>No</td>\n",
       "      <td>0.656752</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>1420.790257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>No</td>\n",
       "      <td>0.690306</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>948.487744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>Self</td>\n",
       "      <td>0.723554</td>\n",
       "      <td>{'max_iter': 10}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>67.905287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>Self</td>\n",
       "      <td>0.728051</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>956.190812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>Self</td>\n",
       "      <td>0.760923</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>332.443303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No</td>\n",
       "      <td>0.765575</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>319.620913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>No</td>\n",
       "      <td>0.770526</td>\n",
       "      <td>{'max_iter': 2}</td>\n",
       "      <td>GridSearchCV(cv=KFold(n_splits=5, random_state...</td>\n",
       "      <td>56.847650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name     balance        f1  \\\n",
       "11                 RandomForest          No  0.003101   \n",
       "9                  RandomForest  Downsample  0.180954   \n",
       "1                       Dummy 1        Self  0.184480   \n",
       "2                       Dummy 1          No  0.184480   \n",
       "10                 RandomForest        Self  0.346738   \n",
       "6                  DecisionTree  Downsample  0.427891   \n",
       "12               LGBMClassifier  Downsample  0.497031   \n",
       "0                       Dummy 1  Downsample  0.533368   \n",
       "3            LogisticRegression  Downsample  0.537301   \n",
       "15  PassiveAggressiveClassifier  Downsample  0.544872   \n",
       "7                  DecisionTree        Self  0.616789   \n",
       "8                  DecisionTree          No  0.656752   \n",
       "14               LGBMClassifier          No  0.690306   \n",
       "16  PassiveAggressiveClassifier        Self  0.723554   \n",
       "13               LGBMClassifier        Self  0.728051   \n",
       "4            LogisticRegression        Self  0.760923   \n",
       "5            LogisticRegression          No  0.765575   \n",
       "17  PassiveAggressiveClassifier          No  0.770526   \n",
       "\n",
       "                                  comments  \\\n",
       "11   {'max_depth': 10, 'n_estimators': 10}   \n",
       "9    {'max_depth': 10, 'n_estimators': 10}   \n",
       "1                                       {}   \n",
       "2                                       {}   \n",
       "10  {'max_depth': 10, 'n_estimators': 100}   \n",
       "6   {'criterion': 'gini', 'max_depth': 20}   \n",
       "12  {'max_depth': 10, 'n_estimators': 100}   \n",
       "0                                       {}   \n",
       "3                                {'C': 10}   \n",
       "15                         {'max_iter': 2}   \n",
       "7   {'criterion': 'gini', 'max_depth': 20}   \n",
       "8   {'criterion': 'gini', 'max_depth': 20}   \n",
       "14  {'max_depth': 10, 'n_estimators': 100}   \n",
       "16                        {'max_iter': 10}   \n",
       "13  {'max_depth': 10, 'n_estimators': 100}   \n",
       "4                                {'C': 10}   \n",
       "5                                {'C': 10}   \n",
       "17                         {'max_iter': 2}   \n",
       "\n",
       "                                                model  learning_time  \n",
       "11  GridSearchCV(cv=KFold(n_splits=5, random_state...    1069.129578  \n",
       "9   GridSearchCV(cv=KFold(n_splits=5, random_state...     156.162205  \n",
       "1   GridSearchCV(cv=KFold(n_splits=5, random_state...       1.259130  \n",
       "2   GridSearchCV(cv=KFold(n_splits=5, random_state...       1.072895  \n",
       "10  GridSearchCV(cv=KFold(n_splits=5, random_state...    1253.016030  \n",
       "6   GridSearchCV(cv=KFold(n_splits=5, random_state...     159.222106  \n",
       "12  GridSearchCV(cv=KFold(n_splits=5, random_state...     198.977727  \n",
       "0   GridSearchCV(cv=KFold(n_splits=5, random_state...       0.290892  \n",
       "3   GridSearchCV(cv=KFold(n_splits=5, random_state...     212.849353  \n",
       "15  GridSearchCV(cv=KFold(n_splits=5, random_state...      10.448578  \n",
       "7   GridSearchCV(cv=KFold(n_splits=5, random_state...    1657.575824  \n",
       "8   GridSearchCV(cv=KFold(n_splits=5, random_state...    1420.790257  \n",
       "14  GridSearchCV(cv=KFold(n_splits=5, random_state...     948.487744  \n",
       "16  GridSearchCV(cv=KFold(n_splits=5, random_state...      67.905287  \n",
       "13  GridSearchCV(cv=KFold(n_splits=5, random_state...     956.190812  \n",
       "4   GridSearchCV(cv=KFold(n_splits=5, random_state...     332.443303  \n",
       "5   GridSearchCV(cv=KFold(n_splits=5, random_state...     319.620913  \n",
       "17  GridSearchCV(cv=KFold(n_splits=5, random_state...      56.847650  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the best model: 17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_list.sort_values('f1'))\n",
    "\n",
    "best_model_index = model_list['f1'].idxmax()\n",
    "print('Index of the best model:', best_model_index)\n",
    "\n",
    "def plot_models_parametr(parametr):\n",
    "    \"\"\"\n",
    "    Function to compare parametrs\n",
    "    \"\"\"\n",
    "    global model_list\n",
    "    sns.barplot(data=model_list, x='model_name', y=parametr, hue='balance')\n",
    "    plt.title(parametr)\n",
    "    plt.ylabel(parametr)\n",
    "    plt.xticks(rotation=-70)\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "plot_models_parametr('f1')\n",
    "plot_models_parametr('learning_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation on test sample and additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is:\n",
      " model_name                             PassiveAggressiveClassifier\n",
      "balance                                                         No\n",
      "f1                                                        0.770526\n",
      "comments                                           {'max_iter': 2}\n",
      "model            GridSearchCV(cv=KFold(n_splits=5, random_state...\n",
      "learning_time                                             56.84765\n",
      "Name: 17, dtype: object \n",
      "\n",
      "\n",
      "Analysis for LogisticRegression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[27467,  1155],\n",
       "       [  614,  2623]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.694\n",
      "Recall: 0.81\n",
      "F1: 0.748\n",
      "accuracy: 0.944\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHpklEQVR4nO3dd3wUdeLG8WeTTYcECBBKQkIVEOlFQKSDgIgVFE6KeCc22sn9QJSmZ/BUzlMBPWnnHSKiiCIIxFMgICJVEVBKwNAhlCS0lN35/cExsKaQhCST3f28X6+8+M5kZvPsGNmHqTbDMAwBAAB4CB+rAwAAABQmyg0AAPAolBsAAOBRKDcAAMCjUG4AAIBHodwAAACPQrkBAAAehXIDAAA8CuUGAAB4FMoNUEzmzZsnm81mftntdkVGRmrIkCE6cuRIsecZPHiwYmJi8rXOwYMHZbPZNG/evCLJdCODBw922Yb+/v6qWbOmnnvuOaWkpFiS6XrZbZ+r/90PHjxoWS7A29itDgB4m7lz56pu3bq6dOmS1q5dq9jYWK1Zs0Y7duxQSEhIseV48cUXNWLEiHytU7lyZW3YsEE1a9YsolQ3FhQUpG+++UaSdO7cOX3yySd644039NNPP2nVqlWW5QJQclBugGLWoEEDNW/eXJLUsWNHORwOvfTSS1qyZIkGDBiQ7ToXL15UcHBwoeYoSEEJCAjQ7bffXqg58svHx8clw1133aWEhATFxcXpwIEDql69uoXpSrZLly4pKCjI6hhAkeOwFGCxqx/Uv/32m6Qrh15KlSqlHTt2qFu3bipdurQ6d+4sSUpPT9fLL7+sunXrKiAgQBUqVNCQIUN06tSpLK/74YcfqnXr1ipVqpRKlSqlxo0ba/bs2eb3szsstWjRIrVq1UphYWEKDg5WjRo19Nhjj5nfz+mw1Lp169S5c2eVLl1awcHBatOmjZYtW+ayzNXDM99++62efPJJlS9fXuHh4br//vt19OjRAm8/SWZZPHHihMv8hQsXqnXr1goJCVGpUqXUvXt3bdu2Lcv6GzduVO/evRUeHq7AwEDVrFlTI0eONL+/b98+DRkyRLVr11ZwcLCqVq2q3r17a8eOHTeV+/d++eUXPfLII4qIiFBAQICqVaumgQMHKi0tTZI0adIk2Wy2LOtld+grJiZGd999txYvXqwmTZooMDBQkydPVpMmTdSuXbssr+FwOFS1alXdf//95rz8/L4BJQnlBrDYvn37JEkVKlQw56Wnp+uee+5Rp06d9Pnnn2vy5MlyOp3q06ePpk6dqv79+2vZsmWaOnWq4uLi1KFDB126dMlcf8KECRowYICqVKmiefPm6bPPPtOgQYPMApWdDRs2qF+/fqpRo4Y++ugjLVu2TBMmTFBmZmau+desWaNOnTopOTlZs2fP1oIFC1S6dGn17t1bCxcuzLL8448/Lj8/P3344Yf629/+ptWrV+sPf/hDfjebiwMHDshut6tGjRrmvFdeeUWPPPKI6tevr48//lj//ve/lZqaqnbt2mnXrl3mcitXrlS7du2UmJioadOm6auvvtILL7zgUpSOHj2q8PBwTZ06VStWrND06dNlt9vVqlUr/frrrzeV/aoff/xRLVq00Pfff68pU6boq6++UmxsrNLS0pSenl6g19y6davGjBmj4cOHa8WKFXrggQc0ZMgQrVu3Tnv37nVZdtWqVTp69KiGDBkiSfn6fQNKHANAsZg7d64hyfj++++NjIwMIzU11fjyyy+NChUqGKVLlzaOHz9uGIZhDBo0yJBkzJkzx2X9BQsWGJKMTz/91GX+pk2bDEnGjBkzDMMwjISEBMPX19cYMGBArnkGDRpkREdHm9Ovv/66Ick4d+5cjuscOHDAkGTMnTvXnHf77bcbFStWNFJTU815mZmZRoMGDYzIyEjD6XS6vP+nnnrK5TX/9re/GZKMY8eO5Zr3auaQkBAjIyPDyMjIMJKSkoyZM2caPj4+xvPPP28ul5iYaNjtduPZZ591WT81NdWoVKmS0bdvX3NezZo1jZo1axqXLl264c+//v2lp6cbtWvXNkaNGmXOz277XH3fBw4cyPU1O3XqZJQpU8Y4efJkjstMnDjRyO6v7ex+RnR0tOHr62v8+uuvLssmJSUZ/v7+LtvLMAyjb9++RkREhJGRkWEYRt5/34CSiD03QDG7/fbb5efnp9KlS+vuu+9WpUqV9NVXXykiIsJluQceeMBl+ssvv1SZMmXUu3dvZWZmml+NGzdWpUqVtHr1aklSXFycHA6Hnn766XzlatGihSSpb9+++vjjj/N0BdeFCxe0ceNGPfjggypVqpQ539fXV48++qgOHz6cZc/GPffc4zLdsGFDSdcOyzmdTpf353A4svxMPz8/+fn5qXz58nryySfVr18//fWvfzWXWblypTIzMzVw4ECX1woMDFT79u3NbbVnzx7t379fQ4cOVWBgYI7vMzMzU6+88orq168vf39/2e12+fv7a+/evdq9e/cNt9ONXLx4UWvWrFHfvn1d9uDdrIYNG6pOnTou88LDw9W7d2/961//ktPplCSdPXtWn3/+uQYOHCi7/cqpmHn9fQNKIsoNUMw++OADbdq0Sdu2bdPRo0f1008/qW3bti7LBAcHKzQ01GXeiRMndO7cOfn7+5sf7le/jh8/rqSkJEkyz4eIjIzMV64777xTS5YsMUtBZGSkGjRooAULFuS4ztmzZ2UYhipXrpzle1WqVJEknT592mV+eHi4y3RAQIAkmYc5pkyZ4vLefn/ic1BQkDZt2qRNmzZp6dKl6tChgxYsWKCpU6eay1w9pNSiRYss22rhwoX53lajR4/Wiy++qHvvvVdLly7Vxo0btWnTJjVq1KhQDs+cPXtWDocj3//NbiS7/y6S9Nhjj+nIkSOKi4uTJC1YsEBpaWkaPHiwuUxef9+AkoirpYBiVq9ePfME2Jxkd9Lo1RNwV6xYke06pUuXlnTt3J3Dhw8rKioqX9n69OmjPn36KC0tTd9//71iY2PVv39/xcTEqHXr1lmWL1u2rHx8fHTs2LEs37t6knD58uXzleFPf/qT7r77bnP6avm5ysfHx2X7de3aVc2aNdPkyZM1YMAARUVFmT/zk08+UXR0dI4/6/ptlZv//Oc/GjhwoF555RWX+UlJSSpTpkye3lduypUrJ19f3xvmuLp3KS0tzWW75FQ0svs9kqTu3burSpUqmjt3rrp37665c+eqVatWql+/vrlMXn/fgJKIcgO4ibvvvlsfffSRHA6HWrVqleNy3bp1k6+vr2bOnJltIcmLgIAAtW/fXmXKlNHKlSu1bdu2bF8rJCRErVq10uLFi/X666+blxk7nU795z//UWRkZJbDIjdSpUoVc69PXrNOnz5dHTp00Msvv6z33ntP3bt3l91u1/79+7Mc3rtenTp1VLNmTc2ZM0ejR4/OUqSustlsWb63bNkyHTlyRLVq1cpz1pwEBQWpffv2WrRokf7617/mWAivXt32008/mYcRJWnp0qX5+nlXDxu++eabio+P1+bNm/Xee++5LJPX3zegJKLcAG7i4Ycf1vz589WzZ0+NGDFCLVu2lJ+fnw4fPqxvv/1Wffr00X333aeYmBg9//zzeumll3Tp0iU98sgjCgsL065du5SUlKTJkydn+/oTJkzQ4cOH1blzZ0VGRurcuXP6xz/+IT8/P7Vv3z7HXLGxseratas6duyo5557Tv7+/poxY4Z+/vlnLViwIMe9B4Wpffv26tmzp+bOnauxY8eqevXqmjJlisaPH6+EhATdddddKlu2rE6cOKEffvhBISEh5naYPn26evfurdtvv12jRo1StWrVlJiYqJUrV2r+/PmSrnzQz5s3T3Xr1lXDhg21ZcsWvfbaa4V6GGnatGm644471KpVK40dO1a1atXSiRMn9MUXX+i9995T6dKl1bNnT5UrV05Dhw7VlClTZLfbNW/ePB06dCjfP++xxx7Tq6++qv79+ysoKEj9+vVz+X5ef9+AEsnqM5oBb3H1ipZNmzblutzVK4Kyk5GRYbz++utGo0aNjMDAQKNUqVJG3bp1jSeeeMLYu3evy7IffPCB0aJFC3O5Jk2auFzF8/urpb788kujR48eRtWqVQ1/f3+jYsWKRs+ePY34+HhzmeyuBjIMw4iPjzc6depkhISEGEFBQcbtt99uLF26NE/v/9tvvzUkGd9++22u2+VG22bHjh2Gj4+PMWTIEHPekiVLjI4dOxqhoaFGQECAER0dbTz44IPG119/7bLuhg0bjB49ehhhYWFGQECAUbNmTZeroM6ePWsMHTrUqFixohEcHGzccccdRnx8vNG+fXujffv2uW6fvF4tZRiGsWvXLuOhhx4ywsPDDX9/f6NatWrG4MGDjcuXL5vL/PDDD0abNm2MkJAQo2rVqsbEiRONWbNmZXu1VK9evXL9eW3atDEk5XhlXX5+34CSxGYYhmFdtQIAAChcXC0FAAA8CuUGAAB4FMoNAADwKJQbAADgUSg3AADAo1BuAACAR/G6m/g5nU4dPXpUpUuXLpabiwEAgJtnGIZSU1NVpUoV+fjkvm/G68rN0aNH8/28HQAAUDIcOnTohncH97pyc/Vhb4cOHcry1GUAAFAypaSkKCoqKk8PbfW6cnP1UFRoaCjlBgAAN5OXU0o4oRgAAHgUyg0AAPAolBsAAOBRKDcAAMCjUG4AAIBHodwAAACPQrkBAAAehXIDAAA8CuUGAAB4FMoNAADwKJaWm7Vr16p3796qUqWKbDablixZcsN11qxZo2bNmikwMFA1atTQu+++W/RBAQCA27C03Fy4cEGNGjXSO++8k6flDxw4oJ49e6pdu3batm2bnn/+eQ0fPlyffvppEScFAADuwtIHZ/bo0UM9evTI8/LvvvuuqlWrpjfffFOSVK9ePW3evFmvv/66HnjggSJKmTcOp6FjyZcszeDtypcKUKCfr9UxAAAWc6ungm/YsEHdunVzmde9e3fNnj1bGRkZ8vPzy7JOWlqa0tLSzOmUlJQiyXb6QpruePXbInlt5N1DzSLlb/eRIckwJMmQ0ykZMmQYMueb04Zx3TzJaRhKy3CqS72KqlOptAzDkNOQnM4rf15d3nl1vmHIMAxVLB2oW6uE5ulptQCAouVW5eb48eOKiIhwmRcREaHMzEwlJSWpcuXKWdaJjY3V5MmTiyVfgJ3zs62SlumUJC3acrhQXu/r3ScKtN7THWtKkhzOK8XH4bzydXXsNAylZxp6oFlVOZ2SwzDkdBrKvG65ymGBalKtbKG8DwDwRm5VbiRl+ZexceWf5zn+i3ncuHEaPXq0OZ2SkqKoqKhCz1WxdKB+fTnvh9hQuDYfPKO1e05JNptskmw2yee6sc1mu/Knrv75v+//79fG9r9lE89c1IcbE1UxNEA+Npt8bNeWc/3z2vd2HEk2c0z/dn+e8n669cYlrPutETIM6ezFdD3aOkYdb6mgTIehDKdTmQ7DHDuchjIcTkWHh6hUgNv9Lw0Ahc6t/iasVKmSjh8/7jLv5MmTstvtCg8Pz3adgIAABQQEFEc8WKh5TDk1jylXKK816Z5b87W802lozvoD2n/qgvx8bfKx2eTrc+Xryljytdnk42PTx5sOyd/uI7uvj+w+15b18bHJ1yZtTTxnvu7Kndf2Hm06eDZPWcoG++nsxQz1aVxFpQLsahZdVpkOQ352m9rWLK+KoYE5rutwGvL14bAaAPfnVuWmdevWWrp0qcu8VatWqXnz5tmebwMUBx8fmx5vVyNPy47sUueGy8TvPaU9J87Lz9emo+cu6901WfcG+fnaZPfxkd3XJruPTWcvZkiS+efn249KkuZvTMyyblS5IGVkGkp3OJWR6bzyp8Mp55WdoBrdtY4yHE4F2H30SMtqKhVoV3qmU+mZToUE2DlpG0CJZzOuHtexwPnz57Vv3z5JUpMmTTRt2jR17NhR5cqVU7Vq1TRu3DgdOXJEH3zwgaQrl4I3aNBATzzxhP74xz9qw4YNGjZsmBYsWJDnq6VSUlIUFham5ORkhYaGFtl7AwqL02noYoZDdp8rRcbXx5blMOyFtEztP3VeW347q3MXM7Tgh0TVqBAiP18fxe9NKvRMnetWVK+GldW+TgWFl2LPKICil5/Pb0vLzerVq9WxY8cs8wcNGqR58+Zp8ODBOnjwoFavXm1+b82aNRo1apR27typKlWq6P/+7/80bNiwPP9Myg281Z4TqTp7IV3+dh/5+fqYf/r52uRv99GMb/cr9XKm/O0+itt1XEnn0/P0um1qhqtr/QhdTHfolojSKhPsp4aRZeTPCfYACpHblBsrUG6AvEm+mKEMp1P+dh/5+/rIMKS/f71H3+1P0s9HbnxLhfqVQ3X6QpoMQ2oRU06XMhzacSRZo7rUUYOqoWoYWabo3wQAj0G5yQXlBrh5Dqehjzcf0hfbj8pmk85cSNcvx1Pz/TpPdqipu26tpIaRYdwjCECuKDe5oNwAReunw+d0LPmyfGw2bT54RuVC/BXo56uVO4/ru/2nc1zPZpP+2K6GIkIDde5iuiqUDlB4SIDa1gpXmWD/YnwHAEoiyk0uKDeAtQzD0N+/3qv/7j6hnUfzfsfwYH9f3d2wsi5lOJV6OUNzB7dgbw/gRSg3uaDcACVHhsOpnUdTNP3bffrleIrqVCytdIczz1d4fT36TlUoHShfHxs3MAQ8HOUmF5QbwH04nIZW/3pS8XuT5OdrU2ign96I25Pj8nfWqaCywX5qEVNOfZtHccUW4EEoN7mg3ADuzeE0dO/09S6PvbiR8qX81bJ6OU3r25ibEAJuinKTC8oN4BkupmfqcoZTIQG+WvrjMcXvPaUyQX7614bfbrjuA00j9UbfRsWQEkBhodzkgnIDeLZMh1OplzOVkHRB+06myjCksYt3ZLvswNbR6tO4iq7+LdikWlmerwWUUJSbXFBuAO+UlunQvPUHFfvVL7kuV79yqEZ1raPLGQ41iiyjauHBxZQQQG4oN7mg3ADeLcPh1OC5P2j9viv33Ln6JPUb6dc8Srf+787KjaPKFHFKAL9HuckF5QZAdvadPK8u09aoapkgHTl3Kddl721cRW8+3KSYkgGQKDe5otwAyKuDSRf02qpftebXUwry99Wp1DSX73etH6HaFUtp6B3VeTo6UMQoN7mg3AAoqL0nUtX172uz/d6nT7ZWs+hyxZwI8B6Um1xQbgDcjAyHU+/HJ+iD737T8ZTLLt+rUSFEFUoFaFiHmup4S0WLEgKeiXKTC8oNgMI06YudmvfdwWy/VzrQrq0vdpXdx8ZzsICbRLnJBeUGQGFbu+eU/vP9b9p08EyuV1493CJKL9xdX8F+vjIk2XTlaegUH+DGKDe5oNwAKGpf7zqhxz/YnK912tUur6kPNFTVMkFFlApwb5SbXFBuABSXk6mXdTIlTUt/PKr31ibkeb09L/fgoZ/A71BuckG5AWCV5EsZynA4lZ7pVMrlDJ27mKFZ8Qf09e4T2S6/8E+3q1WN8GJOCZRMlJtcUG4AlEQOp6Gazy/PMv/ZTrX05263WJAIKFkoN7mg3AAoyRxOQw0nrdSFdIfL/NoVSyludHuLUgHWy8/nNwd1AaAE8fWxaeeUu/T3fo1c5u89eV4xY5dp7voDSs90WpQOcA/suQGAEuzHQ+fUZ/r6bL/XMqacOtStoPubRKpSWGAxJwOKF4elckG5AeBuHE5DL325K8ebBUrSUx1q6i931S2+UEAxo9zkgnIDwJ2dT8vU0h+PatziHdl+/1+PtVT7OhWKORVQ9Cg3uaDcAPAk6/clacCsjVnmrxnTQdHhIRYkAooGJxQDgJdoW6u8Dk7tleXOxu1fW62mL8Vp/sbfLEoGWIc9NwDgIZIvZajR5FXZfm/n5O4KCbAXcyKg8LDnBgC8UFiQnw5O7aU3Hmqk8qUCXL5368SVeunLXbqc4chhbcBzsOcGADzUpXSH6k1YkWX+Iy2jFHt/QwsSAQXHnhsAgIL8fXVwai892aGmy/wFPxzSvzcctCYUUAzYcwMAXuKjHxI19rpLyKuWCdLKUXeqFOfiwA2w5wYAkMXDLatpcJsYc/rIuUtqMHGlYsYuU/zeU9YFAwoZ5QYAvMike27VX+7K+pTxR2f/oJixy5R0Ps2CVEDh4rAUAHiplMsZajgp66Xj34/rzLOqUOJwh+JcUG4AIKuYscuyzKsUGqg1f+mgALuvBYkAV5xzAwDIl4NTe6nK7/bWHE+5rFteWKGTKZctSgUUDOUGACBJ+m5cZ60adadqlHd9JlXLV/6rT7YctigVkH8clgIAZGvc4h1a8EOiy7yDU3tZlAbejsNSAICbFnv/bXr3D81c5sWMXSYv+zcx3BDlBgCQo7saVNIvL93lMq/6uOUWpQHyhnIDAMhVoJ+vtr3Y1WXewk2JOSwNWI9yAwC4obIh/tr71x7m9P99ukP7Tp63MBGQM8oNACBP/Hx99OLd9c3pLtPW6OPNhyxMBGSPcgMAyLOhd1RXzQrXLhX/yyc/acCs7znJGCUK5QYAkC///XMHNah67VLc9ftOq/q45Vq/L8nCVMA1lBsAQL59+Ww7ffZUG5d5A2Zt1OUMh0WJgGsoNwCAAmlSrawSXumpiqUDzHl1X1yhC2mZFqYCKDcAgJvg42PTd2M7ucy7deJKJZziSipYh3IDALgpdl8fHZzaS6UC7Oa8Tm+sUf/3v7cwFbwZ5QYAUCh+ntzd5UTj7/afVszYZTp3Md3CVPBGlBsAQKH58tl2mv94K5d5jafE6bv9XEmF4kO5AQAUqra1yivhlZ4u8/q/v5F74aDYUG4AAIXOx8emg1N76ZmOtcx5t8f+18JE8CaUGwBAkXmu+y3m+ERKmob9e4uFaeAtKDcAgCK177oHbq7YeZyrqFDkKDcAgCJl9/XR5he6mNPf7T+tJlNW6dCZixamgiej3AAAilz5UgFaOfJOc/rsxQy1+9u3+unwOetCwWNRbgAAxeKWSqX1/bjOigkPNufd8856CxPBU1FuAADFplJYoFaP6ahu9SPMec8u2GZhIngiyg0AoNi992gzc7z0x6P65pcTFqaBp6HcAACKnc1m08bnO5vTj83brNTLGRYmgieh3AAALBERGqj3BzY3p2+btMrCNPAklpebGTNmqHr16goMDFSzZs0UHx+f6/Lz589Xo0aNFBwcrMqVK2vIkCE6ffp0MaUFABSmrtedeyNJp1LTLEoCT2JpuVm4cKFGjhyp8ePHa9u2bWrXrp169OihxMTEbJdft26dBg4cqKFDh2rnzp1atGiRNm3apMcff7yYkwMACsv1N/lr8devlZbpsDANPIGl5WbatGkaOnSoHn/8cdWrV09vvvmmoqKiNHPmzGyX//777xUTE6Phw4erevXquuOOO/TEE09o8+bNxZwcAFBY7L6uH0WNJnN4CjfHsnKTnp6uLVu2qFu3bi7zu3Xrpu+++y7bddq0aaPDhw9r+fLlMgxDJ06c0CeffKJevXoVR2QAQBH55aW7zPHlDKdOn+fwFArOsnKTlJQkh8OhiAjX460RERE6fvx4tuu0adNG8+fPV79+/eTv769KlSqpTJkyevvtt3P8OWlpaUpJSXH5AgCULIF+vlo7pqM53ezlr3UhLdPCRHBnlp9QbLPZXKYNw8gy76pdu3Zp+PDhmjBhgrZs2aIVK1bowIEDGjZsWI6vHxsbq7CwMPMrKiqqUPMDAApHtfBgVS0TZE73fmedhWngziwrN+XLl5evr2+WvTQnT57MsjfnqtjYWLVt21ZjxoxRw4YN1b17d82YMUNz5szRsWPHsl1n3LhxSk5ONr8OHTpU6O8FAFA41o/tZI4TTl1QeqbTwjRwV5aVG39/fzVr1kxxcXEu8+Pi4tSmTZts17l48aJ8fFwj+/r6Srqyxyc7AQEBCg0NdfkCAJRcq5/rYI7rvPCVPt9+JMe/44HsWHpYavTo0Zo1a5bmzJmj3bt3a9SoUUpMTDQPM40bN04DBw40l+/du7cWL16smTNnKiEhQevXr9fw4cPVsmVLValSxaq3AQAoRDHlQxQe4m9Oj/hou95dk2BhIrgbu5U/vF+/fjp9+rSmTJmiY8eOqUGDBlq+fLmio6MlSceOHXO5583gwYOVmpqqd955R3/+859VpkwZderUSa+++qpVbwEAUAS2vNhV905fr+2HzkmSXl3xi57sUNPaUHAbNsPL9vWlpKQoLCxMycnJHKICgBLuD7M2at2+JElSs+iy+vTJ7E9bgOfLz+e35VdLAQCQk38OvPb08C2/nbUwCdwJ5QYAUGIF+9u1atSd5nTM2GU6cyHdwkRwB5QbAECJVieitMt005ficlgSuIJyAwAo8a5/PANwI5QbAECJF+jnq5Ujrx2ecjq96loY5BPlBgDgFqLDg81xjeeXa/LSnZQcZItyAwBwC4F+vi7Tc9cfVI3nl1uUBiUZ5QYA4DYOxPZU46gyLvPidp2wJgxKLMoNAMBt2Gw2LXm6rQ7E9jTn/fGDzRYmQklEuQEAuB2bzaZbq3CXeWSPcgMAcEszB1y7e/Hdb8dbmAQlDeUGAOCWosoFmeOfj6Qofu8pC9OgJKHcAADcks1m03djO5nTj87+Qecu8mgGUG4AAG6sSpkgdb81wpxuPIVHM4ByAwBwc+892txl+ol/c/WUt6PcAADc3sGpvczxyp0ntOLnYxamgdUoNwAAj/D16PbmeNh/tmrm6v0WpoGVKDcAAI9Qq2IpTelzqzn96opf5ODZU16JcgMA8BgDW8doVJc65vSHG3+zMA2sQrkBAHiUEV1qm+MXP99pYRJYhXIDAPA4MeHB5vjnI8kWJoEVKDcAAI/z+dN3mOO7314nJ+feeBXKDQDA44QF+6lXw8rmdI3nl1uYBsWNcgMA8EjT+zd1mZ6ydJdFSVDcKDcAAI91/c395qw/IMPg8JQ3oNwAADza0meunX9TfRyHp7wB5QYA4NFuiwxzmebkYs9HuQEAeLyfJnUzx/UmrFCmw2lhGhQ1yg0AwOOFBvqZ47RMp/q/v9HCNChqlBsAgFf4flxnc/zDwTP66IdEC9OgKFFuAABeoVJYoD57qo05PXbxDu5e7KEoNwAAr9GkWln9vV8jc/rut9dZmAZFhXIDAPAq9zWJdJme/u0+i5KgqFBuAABeZ/8rPc3xayt/VQZXT3kUyg0AwOv4+tg0+Z5bzena47/SxfRMCxOhMFFuAABeaVCbGJfp+hNWWhMEhY5yAwDwWgdie7pMX0p3WJQEhYlyAwDwWjabTb++fJc5XW/CCgvToLBQbgAAXi3A7usy/eKSny1KgsJCuQEAeL3rD0/9+/vflJ7J1VPujHIDAPB6NptN/3dXXXO6zgtfWZgGN4tyAwCApD/dWcNl+oMNB60JgptGuQEAQFfuffPTpG7m9ITPdypm7DILE6GgKDcAAPxPaKCf/ty1jss8Co77odwAAHCdZzvX1i8v3eUyb/xnOyxKg4Kg3AAA8DuBfr4uBWf+xkR9vv2IhYmQH5QbAACyEejnqzHdbzGnR3y0XdsSz1qYCHlFuQEAIAdPd6yldrXLm9P3zfjOwjTIK8oNAAC5+OCxli4Fp9bzyy1Mg7yg3AAAkAubzaZ/D21lTmc6DQvTIC8oNwAA5MHq5zqY41U7j1sXBDdEuQEAIA9iyoeY4z/9e4uFSXAjlBsAAPKoabUy5njy0p3WBUGuKDcAAOTRJ8PamOO56w9aFwS5otwAAJBHPj42Pd2xpjm990SqhWmQE8oNAAD58HCLaua469/XWpgEOaHcAACQD1HlglUm2M+c5sqpkodyAwBAPv3wfBdznHjmooVJkB3KDQAA+eRv91GT/1059fKy3Tp67pK1geCCcgMAQAFUD79235s2U79ReqbTwjS4ns0wjHzfR/rChQuaOnWq/vvf/+rkyZNyOl3/gyYkJBRawMKWkpKisLAwJScnKzQ01Oo4AAA3ZRiGqo9zfc7Uwam9LErj+fLz+W0vyA94/PHHtWbNGj366KOqXLmybDZbgYICAOCubDabfp7cXQ0mrjTnnbuYrjLB/hamglTAPTdlypTRsmXL1LZt26LIVKTYcwMAKEzJFzPUaMoqc/pAbE/+0V8E8vP5XaBzbsqWLaty5coVKBwAAJ4k7LrLwiXpm19OWpQEVxWo3Lz00kuaMGGCLl68+cvfZsyYoerVqyswMFDNmjVTfHx8rsunpaVp/Pjxio6OVkBAgGrWrKk5c+bcdA4AAArq+nNthv5rs5zOfB8UQSEq0Dk3b7zxhvbv36+IiAjFxMTIz8+1tW7dujVPr7Nw4UKNHDlSM2bMUNu2bfXee++pR48e2rVrl6pVq5btOn379tWJEyc0e/Zs1apVSydPnlRmZmZB3gYAAIWmdY1wbUg4LUmq8fxy/TihW5a9OigeBTrnZvLkybl+f+LEiXl6nVatWqlp06aaOXOmOa9evXq69957FRsbm2X5FStW6OGHH1ZCQkKBD4txzg0AoChw9VTRys/nd4HKTWFIT09XcHCwFi1apPvuu8+cP2LECG3fvl1r1qzJss5TTz2lPXv2qHnz5vr3v/+tkJAQ3XPPPXrppZcUFBSUp59LuQEAFKXWsf/VseTLkqTJ99yqQW1irA3kIYr8UvCrtmzZot27d8tms6l+/fpq0qRJntdNSkqSw+FQRESEy/yIiAgdP579czoSEhK0bt06BQYG6rPPPlNSUpKeeuopnTlzJsfzbtLS0pSWlmZOp6Sk5DkjAAD59e1zHVT3xRWSpIlf7FTpQLvubxppcSrvUqATik+ePKlOnTqpRYsWGj58uJ555hk1a9ZMnTt31qlTp/L1Wr+/XM4wjBwvoXM6nbLZbJo/f75atmypnj17atq0aZo3b54uXcr+1texsbEKCwszv6KiovKVDwCA/Aj089U/Hm5sTo/++EfrwnipApWbZ599VikpKdq5c6fOnDmjs2fP6ueff1ZKSoqGDx+ep9coX768fH19s+ylOXnyZJa9OVdVrlxZVatWVVhYmDmvXr16MgxDhw8fznadcePGKTk52fw6dOhQHt8lAAAF06dxVQ3vVMucjhm7zMI03qdA5WbFihWaOXOm6tWrZ86rX7++pk+frq+++ipPr+Hv769mzZopLi7OZX5cXJzatGmT7Tpt27bV0aNHdf78eXPenj175OPjo8jI7Hf5BQQEKDQ01OULAICiNrrbLS7Tsct3W5TE+xSo3DidziyXf0uSn59fludM5Wb06NGaNWuW5syZo927d2vUqFFKTEzUsGHDJF3Z6zJw4EBz+f79+ys8PFxDhgzRrl27tHbtWo0ZM0aPPfZYnk8oBgCguOyY1M0cv7c2QTFjl+nshXQLE3mHApWbTp06acSIETp69Kg578iRIxo1apQ6d+6c59fp16+f3nzzTU2ZMkWNGzfW2rVrtXz5ckVHR0uSjh07psTERHP5UqVKKS4uTufOnVPz5s01YMAA9e7dW2+99VZB3gYAAEWqdKCf/vZAQ5d5TV6Ky2FpFJYCXQp+6NAh9enTRz///LOioqJks9mUmJio2267TZ9//nmOh4hKAi4FBwBY4frzbhpUDdWXz7azMI37Kbb73MTFxemXX36RYRiqX7++unTpUtCXKjaUGwCAVa4vOO1ql9f7A5sr0M/XwkTuwy1u4mcVyg0AwCrfJ5zWw//83pzuUi9CswY1tzCR+yiSm/i99dZb+tOf/qTAwMAbnuOS18vBAQDwJrfXCNfMAU315Pwrz2D8evcJixN5pjzvualevbo2b96s8PBwVa9ePecXtNmUkJBQaAELG3tuAABW+2DDQU34fKckadGw1moRU7DnJXoTDkvlgnIDACgJrj//5kBszxzvzo8r8vP5XaBLwX/P4XBo+/btOnv2bGG8HAAAHi/Afu0juPq45cp05P0+cchdgcrNyJEjNXv2bElXis2dd96ppk2bKioqSqtXry7MfAAAeKQtL3Z1ma41Pm93+MeNFajcfPLJJ2rUqJEkaenSpTp48KB++eUXjRw5UuPHjy/UgAAAeKJSAXbtf6WnyzwvO1OkyBSo3CQlJalSpUqSpOXLl+uhhx5SnTp1NHToUO3YsaNQAwIA4Kl8fWyK/0tHc3ru+oPWhfEgBSo3ERER2rVrlxwOh1asWGHevO/ixYvy9eVmRAAA5FVUuWBzPGf9AQuTeI483+fmekOGDFHfvn1VuXJl2Ww2de165bjhxo0bVbdu3UINCACAp6taJkhHzl3S4bOXrI7iEQpUbiZNmqQGDRro0KFDeuihhxQQECBJ8vX11dixYws1IAAAnm501zr686IfJUkOpyFfHy4LvxkFKjeS9OCDD2aZN2jQoJsKAwCAN+pSL8Icn0i5rCplgixM4/54/AIAABYrFXjt4/i+Geu18fmS/yDqkozHLwAAUAI0f/lrJZ1PkySN61FXT7SvaXGikoXHL+SCcgMAKIl+PpKsu99eZ04vG36Hbq0SZmGikqXYH78AAABuToOqYfp69J3mdK+31ilm7DJl8FiGfCtQuXnwwQc1derULPNfe+01PfTQQzcdCgAAb1SrYmnd36Sqy7zRH/9oURr3VaBys2bNGvXq1SvL/Lvuuktr16696VAAAHiraf0a690/NDWnl/54VE6nV51BctMKVG7Onz8vf3//LPP9/PyUkpJy06EAAPBmdzWorHf6NzGnR3283bowbqhA5aZBgwZauHBhlvkfffSR6tevf9OhAADwdnc3rGKOP99+1MIk7qdAN/F78cUX9cADD2j//v3q1KmTJOm///2vFixYoEWLFhVqQAAAvNV9Tarqs21HJEmnz6cpvFSAxYncQ4H23Nxzzz1asmSJ9u3bp6eeekp//vOfdfjwYX399de69957CzkiAADeaUqfW83xhxsTLUziXgr8+IVevXple1IxAAAoHKUD/eTrY5PDaShu9wk927m21ZHcQoHvc3Pu3DnNmjVLzz//vM6cOSNJ2rp1q44cOVJo4QAA8HYV/nco6qfDyTpzId3iNO6hQOXmp59+Up06dfTqq6/qtdde07lz5yRJn332mcaNG1eY+QAA8Gr/eLixOW76Upx1QdxIgcrN6NGjNXjwYO3du1eBgYHm/B49enCfGwAAClGrGuEu01721KQCKVC52bRpk5544oks86tWrarjx4/fdCgAAHDNjxO6meMtv521MIl7KFC5CQwMzPZmfb/++qsqVKhw06EAAMA1YcF+5vjBdzdYmMQ9FKjc9OnTR1OmTFFGRoYkyWazKTExUWPHjtUDDzxQqAEBAICrTB6mmasClZvXX39dp06dUsWKFXXp0iW1b99etWrVUunSpfXXv/61sDMCAOD1Vj/XwRzHfvWLdUHcQIHucxMaGqp169bpm2++0datW+V0OtW0aVN16dKlsPMBAABJMeVDzPHsdQf04t087ign+S43mZmZCgwM1Pbt29WpUyfz8QsAAKBoNY8uq83/O6HYMAzZbDaLE5VM+T4sZbfbFR0dLYfDURR5AABADl6+r4E5TrmcaWGSkq1A59y88MILGjdunHlnYgAAUPRqVyxtjhtNXmVhkpKtQOfcvPXWW9q3b5+qVKmi6OhohYSEuHx/69athRIOAABc4+vjehgq0+GU3bfAT1LyWAUqN/fee69sNht3SQQAoJit+7+OuuPVbyVJIxZu1/T+TS1OVPLkq9xcvHhRY8aM0ZIlS5SRkaHOnTvr7bffVvny5YsqHwAAuE5k2WBzvOynY3rrYSPLHh1vl699WRMnTtS8efPUq1cvPfLII/r666/15JNPFlU2AACQjblDWpjjmav3WZikZMrXnpvFixdr9uzZevjhhyVJAwYMUNu2beVwOOTr61skAQEAgKuOt1Q0x6+v2qOnO9bisvDr5GvPzaFDh9SuXTtzumXLlrLb7Tp69GihBwMAADm7u2Flc1x93HILk5Q8+So3DodD/v7+LvPsdrsyM7nWHgCA4vT2I01cpo8nX7YoScmTr8NShmFo8ODBCggIMOddvnxZw4YNc7kcfPHixYWXEAAAZGGz2ZTwSk/VeP7KXps56w/o+Z71LE5VMuSr3AwaNCjLvD/84Q+FFgYAAOSdj49NZYP9dPZihv65NkEjOtdWSECB7vLiUWyGl92sJiUlRWFhYUpOTlZoaKjVcQAAuCmvrvhFM1fvN6cPTu1lYZqik5/Pb25rCACAGxvT7RaX6csZPPuRcgMAgBvz8bHpxwndzOm6L67w+icIUG4AAHBzYcF+LtNjPvnJoiQlA+UGAAAPsGl8F3P8yZbDFiaxHuUGAAAPUKF0gIZ3qmVO7zt53sI01qLcAADgIUZ2qWOO73lnnYVJrEW5AQDAQ/hc93Twi+nee9UU5QYAAA8yc0BTc/zXZbssTGIdyg0AAB6kx23XHqj5fvwBC5NYh3IDAICH+dOdNcyxN97Uj3IDAICHGdP92l2Ll+84ZmESa1BuAADwMH6+1z7eR3/8o4VJrEG5AQDAA0WHB5tjb3scA+UGAAAPtOiJ1ua4xz/iLUxS/Cg3AAB4oIqhgea4fuVQC5MUP8oNAAAe6uqJxUt/OmpxkuJFuQEAwENdPdcm0M/X4iTFi3IDAICHiix75aTi1MuZSsv0nvvdUG4AAPBQXepHmOPbJq6yMEnxsrzczJgxQ9WrV1dgYKCaNWum+Pi8ndG9fv162e12NW7cuGgDAgDgpkoF2M1xusNpYZLiZWm5WbhwoUaOHKnx48dr27ZtateunXr06KHExMRc10tOTtbAgQPVuXPnYkoKAIB7evuRJuY48fRFC5MUH0vLzbRp0zR06FA9/vjjqlevnt58801FRUVp5syZua73xBNPqH///mrdunWuywEA4O263Xrt0JS3XDVlWblJT0/Xli1b1K1bN5f53bp103fffZfjenPnztX+/fs1ceLEPP2ctLQ0paSkuHwBAOAtAuzXrpQK8pKrpiwrN0lJSXI4HIqIiHCZHxERoePHj2e7zt69ezV27FjNnz9fdrs922V+LzY2VmFhYeZXVFTUTWcHAMCd9G5URZJ0yUueEG75CcU2m81l2jCMLPMkyeFwqH///po8ebLq1KmT59cfN26ckpOTza9Dhw7ddGYAANzJhbRMSdJrK3+1OEnxyNvujyJQvnx5+fr6ZtlLc/LkySx7cyQpNTVVmzdv1rZt2/TMM89IkpxOpwzDkN1u16pVq9SpU6cs6wUEBCggIKBo3gQAAG7garnxFpbtufH391ezZs0UFxfnMj8uLk5t2rTJsnxoaKh27Nih7du3m1/Dhg3TLbfcou3bt6tVq1bFFR0AALfy/qDm5viNVZ6/98ayPTeSNHr0aD366KNq3ry5WrdurX/+859KTEzUsGHDJF05pHTkyBF98MEH8vHxUYMGDVzWr1ixogIDA7PMBwAA14QG+pnjt7/Zp/6tqqlyWJCFiYqWpefc9OvXT2+++aamTJmixo0ba+3atVq+fLmio6MlSceOHbvhPW8AAMCNTepd3xy3jv3GwiRFz2ZcfaqWl0hJSVFYWJiSk5MVGupdj4AHAHi32yatVOrlK+ffHJzay+I0+ZOfz2/Lr5YCAADF4/On25rj+L2nLExStCg3AAB4ierlQ8zxo7N/sDBJ0aLcAADgJWw2m6qVCzanz11MtzBN0aHcAADgRf4z9NqtU4bM22RhkqJDuQEAwItUC7+252Zb4jl54nVFlBsAALzMu39oao4vpHve86YoNwAAeJnO9a495mj6t/ssTFI0KDcAAHgZP99rH/87j6ZYmKRoUG4AAPBCd9apIElau+eU0jI969AU5QYAAC/0l+63mONlPx2zMEnho9wAAOCFGlQNM8ejP/7Ro66aotwAAOClospdezL4qyt+tTBJ4aLcAADgpb79cwdz/O6a/R6z94ZyAwCAl7L7+ujVB24zp7v+fa2FaQoP5QYAAC/2QNNIc7zv5Hk5nO6/94ZyAwCAF7P7+ij+Lx3N6ftnrLcwTeGg3AAA4OWirntS+K5j7n9TP8oNAADQX+66ct+bDIfh9icWU24AAIBa1wg3x88s2GZhkptHuQEAAGpSraw5dvc7FlNuAACAJGnMdY9kOJFy2cIkN4dyAwAAJElPdahpjo8lU24AAICbs9ls5njNr6csTHJzKDcAACCLIH/3rQjumxwAABS6nrdVkiS9svwXi5MUHOUGAACYfjqcbI4zHE4LkxQc5QYAAJj+++f25njvifMWJik4yg0AADAF2H3N8bjFP1mYpOAoNwAAIFs/Hk6W0w2fEk65AQAALtaM6WCOtySetS5IAVFuAACAi+jwEHP80LsbLExSMJQbAACQq+RLGVZHyBfKDQAAyOKH8Z3N8YMzv7MwSf5RbgAAQBYVSwea470n3euScMoNAADI1pzBzc3xpXSHhUnyh3IDAACydUetCub4yLmLFibJH8oNAADIlr/9Wk1Y7UZPCafcAACAHPn7XqkK8zcmWpwk7yg3AAAgR6FBfpKkA0kXLE6Sd5QbAACQo+e61bE6Qr5RbgAAQI6aRZc1x2cupFuYJO8oNwAAIEe1I0qb429+OWlhkryj3AAAgDxJPO0e591QbgAAQK5axpSTJH2545jFSfKGcgMAAHKVcvnKgzMTTrHnBgAAeIBHW0ebY8MwLEySN5QbAACQqz6Nq5rj5TuOW5gkbyg3AAAgV6UC7Ob47W/2Wpgkbyg3AADghu5tXEWS5OtjszjJjVFuAADADV29md/Ooykl/rwbyg0AALihdrUrmOMj5y5ZmOTGKDcAAOCGYsqHmONZ8QcsTHJjlBsAAJAv8747aHWEXFFuAABAnvzpzhrmOPlShoVJcke5AQAAefJ8z3rmePexFAuT5I5yAwAA8szffqU6fPHjUYuT5IxyAwAA8iw90ylJ+nBjosVJcka5AQAAedbrtsqSJHsJvpkf5QYAAORZn//dqTjTaSgt02FxmuxRbgAAQJ61rF7OHL+24lcLk+SMcgMAAPKsTLC/Of7vLyctTJIzyg0AAMiXLvUiJEkHki4o0+G0OE1WlBsAAJAvI7vUNseLthy2MEn2KDcAACBfGlQNM8cHT1+wMEn2LC83M2bMUPXq1RUYGKhmzZopPj4+x2UXL16srl27qkKFCgoNDVXr1q21cuXKYkwLAAAkqVFUGUnSr8dTrQ2SDUvLzcKFCzVy5EiNHz9e27ZtU7t27dSjRw8lJmZ/Y6C1a9eqa9euWr58ubZs2aKOHTuqd+/e2rZtWzEnBwDAu/n+7zY3q389JcMwrA3zOzbDwkStWrVS06ZNNXPmTHNevXr1dO+99yo2NjZPr3HrrbeqX79+mjBhQp6WT0lJUVhYmJKTkxUaGlqg3AAAeLu1e05p4JwfJEl7Xu5hPpahqOTn89uyPTfp6enasmWLunXr5jK/W7du+u677/L0Gk6nU6mpqSpXrlyOy6SlpSklJcXlCwAA3Jxm0WXNcfzeUxYmycqycpOUlCSHw6GIiAiX+RERETp+/HieXuONN97QhQsX1Ldv3xyXiY2NVVhYmPkVFRV1U7kBAIAUEmA3x59uLVlXTFl+QrHN5vpsCsMwsszLzoIFCzRp0iQtXLhQFStWzHG5cePGKTk52fw6dOjQTWcGAABSzQohkqTLGSXrXjf2Gy9SNMqXLy9fX98se2lOnjyZZW/O7y1cuFBDhw7VokWL1KVLl1yXDQgIUEBAwE3nBQAArjreUlH7Tx3QNyXsTsWW7bnx9/dXs2bNFBcX5zI/Li5Obdq0yXG9BQsWaPDgwfrwww/Vq1evoo4JAABycFtk2I0XsoClh6VGjx6tWbNmac6cOdq9e7dGjRqlxMREDRs2TNKVQ0oDBw40l1+wYIEGDhyoN954Q7fffruOHz+u48ePKzk52aq3AACA17r+Zn6fbSs5591YWm769eunN998U1OmTFHjxo21du1aLV++XNHR0ZKkY8eOudzz5r333lNmZqaefvppVa5c2fwaMWKEVW8BAACvVbNCKXO8bu9pC5O4svQ+N1bgPjcAABSeTm+sVsKpK49gODi16E4XcYv73AAAAPfXtNqV+91ULx9icZJrKDcAAKDAHmgaKUk6kHShxDyGgXIDAAAKLNDvWpWYs/6gdUGuQ7kBAAAF1jCyjDned/K8dUGuQ7kBAAAF5utjU6+GlSVJCacoNwAAwANcPddm08EzFie5gnIDAABuSt1KVy7NdpaM84kpNwAA4OY0jylrjpMvZViY5ArKDQAAuCnNo8uZ4693nbAwyRWUGwAAcFP87dfqREk4MkW5AQAAN619nQqSpA83/mZxEsoNAAAoBGv2nJIkbU08Z20QUW4AAEAhePHu+pKk0oF2i5NQbgAAQCFoX6e8JCn1cqbFSSg3AACgENh9rlWKtEyHhUkoNwAAoBBUKRNkji9nOC1MQrkBAACFwGazOsE1lBsAAHDTfK5rN4mnL1qYhHIDAAAKga/PtXLz6opfLExCuQEAAIXM6svBKTcAAKBQPN2xpiRp3d4kS3NQbgAAQKHw/d95N/Uqh1qag3IDAAAKxdVSs/m3M5bmoNwAAIBCke64cn8bu6+19YJyAwAACkXNCqUkSWWC/CzNQbkBAACFwqeE3MmPcgMAADwK5QYAAHgUyg0AAPAolBsAAOBRKDcAAKBQ2GxSgN1HAX7W1gtrH/4AAAA8Rr3Kofr15R5Wx2DPDQAA8CyUGwAA4FEoNwAAwKNQbgAAgEeh3AAAAI9CuQEAAB6FcgMAADwK5QYAAHgUyg0AAPAolBsAAOBRKDcAAMCjUG4AAIBHodwAAACPQrkBAAAexW51gOJmGIYkKSUlxeIkAAAgr65+bl/9HM+N15Wb1NRUSVJUVJTFSQAAQH6lpqYqLCws12VsRl4qkAdxOp06evSoSpcuLZvNVqivnZKSoqioKB06dEihoaGF+tq4hu1cPNjOxYPtXHzY1sWjqLazYRhKTU1VlSpV5OOT+1k1XrfnxsfHR5GRkUX6M0JDQ/kfpxiwnYsH27l4sJ2LD9u6eBTFdr7RHpurOKEYAAB4FMoNAADwKJSbQhQQEKCJEycqICDA6igeje1cPNjOxYPtXHzY1sWjJGxnrzuhGAAAeDb23AAAAI9CuQEAAB6FcgMAADwK5QYAAHgUyk0+zZgxQ9WrV1dgYKCaNWum+Pj4XJdfs2aNmjVrpsDAQNWoUUPvvvtuMSV1b/nZzosXL1bXrl1VoUIFhYaGqnXr1lq5cmUxpnVf+f19vmr9+vWy2+1q3Lhx0Qb0EPndzmlpaRo/fryio6MVEBCgmjVras6cOcWU1n3ldzvPnz9fjRo1UnBwsCpXrqwhQ4bo9OnTxZTWPa1du1a9e/dWlSpVZLPZtGTJkhuuY8nnoIE8++ijjww/Pz/j/fffN3bt2mWMGDHCCAkJMX777bdsl09ISDCCg4ONESNGGLt27TLef/99w8/Pz/jkk0+KObl7ye92HjFihPHqq68aP/zwg7Fnzx5j3Lhxhp+fn7F169ZiTu5e8rudrzp37pxRo0YNo1u3bkajRo2KJ6wbK8h2vueee4xWrVoZcXFxxoEDB4yNGzca69evL8bU7ie/2zk+Pt7w8fEx/vGPfxgJCQlGfHy8ceuttxr33ntvMSd3L8uXLzfGjx9vfPrpp4Yk47PPPst1eas+Byk3+dCyZUtj2LBhLvPq1q1rjB07Ntvl//KXvxh169Z1mffEE08Yt99+e5Fl9AT53c7ZqV+/vjF58uTCjuZRCrqd+/XrZ7zwwgvGxIkTKTd5kN/t/NVXXxlhYWHG6dOniyOex8jvdn7ttdeMGjVquMx76623jMjIyCLL6GnyUm6s+hzksFQepaena8uWLerWrZvL/G7duum7777Ldp0NGzZkWb579+7avHmzMjIyiiyrOyvIdv49p9Op1NRUlStXrigieoSCbue5c+dq//79mjhxYlFH9AgF2c5ffPGFmjdvrr/97W+qWrWq6tSpo+eee06XLl0qjshuqSDbuU2bNjp8+LCWL18uwzB04sQJffLJJ+rVq1dxRPYaVn0Oet2DMwsqKSlJDodDERERLvMjIiJ0/PjxbNc5fvx4tstnZmYqKSlJlStXLrK87qog2/n33njjDV24cEF9+/YtiogeoSDbee/evRo7dqzi4+Nlt/NXR14UZDsnJCRo3bp1CgwM1GeffaakpCQ99dRTOnPmDOfd5KAg27lNmzaaP3+++vXrp8uXLyszM1P33HOP3n777eKI7DWs+hxkz00+2Ww2l2nDMLLMu9Hy2c2Hq/xu56sWLFigSZMmaeHChapYsWJRxfMYed3ODodD/fv31+TJk1WnTp3iiucx8vP77HQ6ZbPZNH/+fLVs2VI9e/bUtGnTNG/ePPbe3EB+tvOuXbs0fPhwTZgwQVu2bNGKFSt04MABDRs2rDiiehUrPgf551celS9fXr6+vln+FXDy5MksrfSqSpUqZbu83W5XeHh4kWV1ZwXZzlctXLhQQ4cO1aJFi9SlS5eijOn28rudU1NTtXnzZm3btk3PPPOMpCsfwoZhyG63a9WqVerUqVOxZHcnBfl9rly5sqpWraqwsDBzXr169WQYhg4fPqzatWsXaWZ3VJDtHBsbq7Zt22rMmDGSpIYNGyokJETt2rXTyy+/zJ71QmLV5yB7bvLI399fzZo1U1xcnMv8uLg4tWnTJtt1WrdunWX5VatWqXnz5vLz8yuyrO6sINtZurLHZvDgwfrwww85Zp4H+d3OoaGh2rFjh7Zv325+DRs2TLfccou2b9+uVq1aFVd0t1KQ3+e2bdvq6NGjOn/+vDlvz5498vHxUWRkZJHmdVcF2c4XL16Uj4/rR6Cvr6+ka3sWcPMs+xws0tOVPczVSw1nz55t7Nq1yxg5cqQREhJiHDx40DAMwxg7dqzx6KOPmstfvQRu1KhRxq5du4zZs2dzKXge5Hc7f/jhh4bdbjemT59uHDt2zPw6d+6cVW/BLeR3O/8eV0vlTX63c2pqqhEZGWk8+OCDxs6dO401a9YYtWvXNh5//HGr3oJbyO92njt3rmG3240ZM2YY+/fvN9atW2c0b97caNmypVVvwS2kpqYa27ZtM7Zt22ZIMqZNm2Zs27bNvOS+pHwOUm7yafr06UZ0dLTh7+9vNG3a1FizZo35vUGDBhnt27d3WX716tVGkyZNDH9/fyMmJsaYOXNmMSd2T/nZzu3btzckZfkaNGhQ8Qd3M/n9fb4e5Sbv8rudd+/ebXTp0sUICgoyIiMjjdGjRxsXL14s5tTuJ7/b+a233jLq169vBAUFGZUrVzYGDBhgHD58uJhTu5dvv/02179vS8rnoM0w2P8GAAA8B+fcAAAAj0K5AQAAHoVyAwAAPArlBgAAeBTKDQAA8CiUGwAA4FEoNwAAwKNQbgBAUkxMjN58801z2mazacmSJZblAVBwlBsAlhs8eLBsNptsNpvsdruqVaumJ598UmfPnrU6GgA3RLkBUCLcddddOnbsmA4ePKhZs2Zp6dKleuqpp6yOBcANUW4AlAgBAQGqVKmSIiMj1a1bN/Xr10+rVq0yvz937lzVq1dPgYGBqlu3rmbMmOGy/uHDh/Xwww+rXLlyCgkJUfPmzbVx40ZJ0v79+9WnTx9FRESoVKlSatGihb7++utifX8Aio/d6gAA8HsJCQlasWKF/Pz8JEnvv/++Jk6cqHfeeUdNmjTRtm3b9Mc//lEhISEaNGiQzp8/r/bt26tq1ar64osvVKlSJW3dulVOp1OSdP78efXs2VMvv/yyAgMD9a9//Uu9e/fWr7/+qmrVqln5VgEUAcoNgBLhyy+/VKlSpeRwOHT58mVJ0rRp0yRJL730kt544w3df//9kqTq1atr165deu+99zRo0CB9+OGHOnXqlDZt2qRy5cpJkmrVqmW+dqNGjdSoUSNz+uWXX9Znn32mL774Qs8880xxvUUAxYRyA6BE6Nixo2bOnKmLFy9q1qxZ2rNnj5599lmdOnVKhw4d0tChQ/XHP/7RXD4zM1NhYWGSpO3bt6tJkyZmsfm9CxcuaPLkyfryyy919OhRZWZm6tKlS0pMTCyW9wageFFuAJQIISEh5t6Wt956Sx07dtTkyZPNPSvvv/++WrVq5bKOr6+vJCkoKCjX1x4zZoxWrlyp119/XbVq1VJQUJAefPBBpaenF8E7AWA1yg2AEmnixInq0aOHnnzySVWtWlUJCQkaMGBAtss2bNhQs2bN0pkzZ7LdexMfH6/Bgwfrvvvuk3TlHJyDBw8WZXwAFuJqKQAlUocOHXTrrbfqlVde0aRJkxQbG6t//OMf2rNnj3bs2KG5c+ea5+Q88sgjqlSpku69916tX79eCQkJ+vTTT7VhwwZJV86/Wbx4sbZv364ff/xR/fv3N082BuB5KDcASqzRo0fr/fffV/fu3TVr1izNmzdPt912m9q3b6958+apevXqkiR/f3+tWrVKFStWVM+ePXXbbbdp6tSp5mGrv//97ypbtqzatGmj3r17q3v37mratKmVbw1AEbIZhmFYHQIAAKCwsOcGAAB4FMoNAADwKJQbAADgUSg3AADAo1BuAACAR6HcAAAAj0K5AQAAHoVyAwAAPArlBgAAeBTKDQAA8CiUGwAA4FEoNwAAwKP8P5AoOnZzdszIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.967\n",
      "{'C': 10, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print('The best model is:\\n', model_list.loc[best_model_index], '\\n\\n')\n",
    "\n",
    "predict_and_metrics_on_test(model_list.loc[4, 'model_name'],\n",
    "                            model_list.loc[4, 'model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Preliminary work has been done on the data - text cleaning and vectorization.\n",
    "Several ML models were trained on the vectorized data, the best turned out to be linear model. Logistic regression and Passive-Agressive Algorithm both successfully met the customer's f1_score requirements on the test sample.\n",
    "Random Forest, Decisive Tree and Busting models performed noticeably worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 523,
    "start_time": "2023-09-12T16:12:15.483Z"
   },
   {
    "duration": 3263,
    "start_time": "2023-09-12T16:13:30.220Z"
   },
   {
    "duration": 1353,
    "start_time": "2023-09-12T16:13:48.233Z"
   },
   {
    "duration": 1327,
    "start_time": "2023-09-12T16:14:52.659Z"
   },
   {
    "duration": 1383,
    "start_time": "2023-09-12T16:16:20.947Z"
   },
   {
    "duration": 1312,
    "start_time": "2023-09-12T16:16:37.382Z"
   },
   {
    "duration": 1312,
    "start_time": "2023-09-12T16:16:45.831Z"
   },
   {
    "duration": 1893,
    "start_time": "2023-09-12T16:17:37.924Z"
   },
   {
    "duration": 2280,
    "start_time": "2023-09-12T16:33:01.634Z"
   },
   {
    "duration": 3082,
    "start_time": "2023-09-12T16:33:03.919Z"
   },
   {
    "duration": 55,
    "start_time": "2023-09-12T16:33:26.788Z"
   },
   {
    "duration": 1745,
    "start_time": "2023-09-12T16:33:33.098Z"
   },
   {
    "duration": 2807,
    "start_time": "2023-09-12T16:33:37.560Z"
   },
   {
    "duration": 1753,
    "start_time": "2023-09-12T16:33:50.668Z"
   },
   {
    "duration": 2891,
    "start_time": "2023-09-12T16:33:52.424Z"
   },
   {
    "duration": 9424,
    "start_time": "2023-09-13T15:05:04.384Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.812Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.813Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.815Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.817Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.818Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.820Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.821Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.823Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.824Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.844Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.846Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:13.848Z"
   },
   {
    "duration": 3808,
    "start_time": "2023-09-13T15:05:24.514Z"
   },
   {
    "duration": 2586,
    "start_time": "2023-09-13T15:05:28.325Z"
   },
   {
    "duration": 292,
    "start_time": "2023-09-13T15:05:30.913Z"
   },
   {
    "duration": 1867,
    "start_time": "2023-09-13T15:05:31.207Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.076Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.077Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.079Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.080Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.081Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.082Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.083Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.085Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T15:05:33.085Z"
   },
   {
    "duration": 3743,
    "start_time": "2023-09-13T15:06:18.396Z"
   },
   {
    "duration": 1143,
    "start_time": "2023-09-13T15:06:22.141Z"
   },
   {
    "duration": 37821,
    "start_time": "2023-09-13T15:06:23.286Z"
   },
   {
    "duration": 10220,
    "start_time": "2023-09-13T15:07:01.110Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T15:07:11.332Z"
   },
   {
    "duration": 75,
    "start_time": "2023-09-13T15:07:11.343Z"
   },
   {
    "duration": 11,
    "start_time": "2023-09-13T15:07:11.420Z"
   },
   {
    "duration": 64,
    "start_time": "2023-09-13T15:07:11.433Z"
   },
   {
    "duration": 2308,
    "start_time": "2023-09-13T15:07:11.499Z"
   },
   {
    "duration": 41321,
    "start_time": "2023-09-13T15:11:29.594Z"
   },
   {
    "duration": 310,
    "start_time": "2023-09-13T15:12:10.917Z"
   },
   {
    "duration": 331,
    "start_time": "2023-09-13T15:12:11.229Z"
   },
   {
    "duration": 1650,
    "start_time": "2023-09-13T15:12:18.448Z"
   },
   {
    "duration": 1210,
    "start_time": "2023-09-13T15:12:20.100Z"
   },
   {
    "duration": 38544,
    "start_time": "2023-09-13T15:12:21.313Z"
   },
   {
    "duration": 10409,
    "start_time": "2023-09-13T15:12:59.859Z"
   },
   {
    "duration": 5,
    "start_time": "2023-09-13T15:13:10.270Z"
   },
   {
    "duration": 71,
    "start_time": "2023-09-13T15:13:10.276Z"
   },
   {
    "duration": 12,
    "start_time": "2023-09-13T15:13:10.349Z"
   },
   {
    "duration": 51,
    "start_time": "2023-09-13T15:13:10.363Z"
   },
   {
    "duration": 506259,
    "start_time": "2023-09-13T15:13:10.415Z"
   },
   {
    "duration": 578176,
    "start_time": "2023-09-13T15:21:36.676Z"
   },
   {
    "duration": 850610,
    "start_time": "2023-09-13T15:31:14.854Z"
   },
   {
    "duration": 329,
    "start_time": "2023-09-13T15:45:25.466Z"
   },
   {
    "duration": 331,
    "start_time": "2023-09-13T15:45:25.797Z"
   },
   {
    "duration": 1525,
    "start_time": "2023-09-13T16:56:19.159Z"
   },
   {
    "duration": 1168,
    "start_time": "2023-09-13T16:56:20.686Z"
   },
   {
    "duration": 1493,
    "start_time": "2023-09-13T16:56:21.856Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-13T16:56:23.352Z"
   },
   {
    "duration": 12,
    "start_time": "2023-09-13T16:56:23.377Z"
   },
   {
    "duration": 18,
    "start_time": "2023-09-13T16:56:23.391Z"
   },
   {
    "duration": 33,
    "start_time": "2023-09-13T16:56:23.410Z"
   },
   {
    "duration": 36,
    "start_time": "2023-09-13T16:56:23.445Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:56:23.483Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:56:23.485Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:56:23.486Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:56:23.487Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:56:23.488Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:56:23.489Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:56:23.490Z"
   },
   {
    "duration": 1525,
    "start_time": "2023-09-13T16:57:24.740Z"
   },
   {
    "duration": 1170,
    "start_time": "2023-09-13T16:57:26.267Z"
   },
   {
    "duration": 1523,
    "start_time": "2023-09-13T16:57:27.443Z"
   },
   {
    "duration": 24,
    "start_time": "2023-09-13T16:57:28.968Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T16:57:28.994Z"
   },
   {
    "duration": 11,
    "start_time": "2023-09-13T16:57:28.999Z"
   },
   {
    "duration": 13,
    "start_time": "2023-09-13T16:57:29.011Z"
   },
   {
    "duration": 28,
    "start_time": "2023-09-13T16:57:29.026Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:57:29.056Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:57:29.057Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:57:29.058Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:57:29.059Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:57:29.060Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:57:29.062Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:57:29.063Z"
   },
   {
    "duration": 1579,
    "start_time": "2023-09-13T16:58:01.806Z"
   },
   {
    "duration": 1238,
    "start_time": "2023-09-13T16:58:03.387Z"
   },
   {
    "duration": 1468,
    "start_time": "2023-09-13T16:58:04.627Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-13T16:58:06.097Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T16:58:06.122Z"
   },
   {
    "duration": 18,
    "start_time": "2023-09-13T16:58:06.127Z"
   },
   {
    "duration": 34,
    "start_time": "2023-09-13T16:58:06.146Z"
   },
   {
    "duration": 54,
    "start_time": "2023-09-13T16:58:06.181Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:58:06.236Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:58:06.237Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:58:06.238Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:58:06.239Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:58:06.240Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:58:06.241Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:58:06.242Z"
   },
   {
    "duration": 1583,
    "start_time": "2023-09-13T16:58:58.250Z"
   },
   {
    "duration": 1226,
    "start_time": "2023-09-13T16:58:59.834Z"
   },
   {
    "duration": 1761,
    "start_time": "2023-09-13T16:59:01.061Z"
   },
   {
    "duration": 75,
    "start_time": "2023-09-13T16:59:02.824Z"
   },
   {
    "duration": 63,
    "start_time": "2023-09-13T16:59:02.901Z"
   },
   {
    "duration": 155,
    "start_time": "2023-09-13T16:59:02.966Z"
   },
   {
    "duration": 69,
    "start_time": "2023-09-13T16:59:03.123Z"
   },
   {
    "duration": 68,
    "start_time": "2023-09-13T16:59:03.194Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:59:03.264Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:59:03.265Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:59:03.266Z"
   },
   {
    "duration": 1,
    "start_time": "2023-09-13T16:59:03.267Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:59:03.269Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:59:03.270Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T16:59:03.271Z"
   },
   {
    "duration": 400,
    "start_time": "2023-09-13T17:01:17.932Z"
   },
   {
    "duration": 255,
    "start_time": "2023-09-13T17:01:30.544Z"
   },
   {
    "duration": 234,
    "start_time": "2023-09-13T17:01:46.290Z"
   },
   {
    "duration": 153,
    "start_time": "2023-09-13T17:01:56.306Z"
   },
   {
    "duration": 339,
    "start_time": "2023-09-13T17:02:12.504Z"
   },
   {
    "duration": 1564,
    "start_time": "2023-09-13T17:02:18.405Z"
   },
   {
    "duration": 1183,
    "start_time": "2023-09-13T17:02:19.971Z"
   },
   {
    "duration": 1671,
    "start_time": "2023-09-13T17:02:21.156Z"
   },
   {
    "duration": 75,
    "start_time": "2023-09-13T17:02:22.829Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:02:22.905Z"
   },
   {
    "duration": 26,
    "start_time": "2023-09-13T17:02:22.910Z"
   },
   {
    "duration": 49,
    "start_time": "2023-09-13T17:02:22.938Z"
   },
   {
    "duration": 418,
    "start_time": "2023-09-13T17:02:22.989Z"
   },
   {
    "duration": 14512,
    "start_time": "2023-09-13T17:02:23.409Z"
   },
   {
    "duration": 6368,
    "start_time": "2023-09-13T17:02:37.924Z"
   },
   {
    "duration": 34033,
    "start_time": "2023-09-13T17:02:44.293Z"
   },
   {
    "duration": 123617,
    "start_time": "2023-09-13T17:03:18.328Z"
   },
   {
    "duration": 2048,
    "start_time": "2023-09-13T17:05:21.947Z"
   },
   {
    "duration": 563,
    "start_time": "2023-09-13T17:05:23.997Z"
   },
   {
    "duration": 335,
    "start_time": "2023-09-13T17:05:24.562Z"
   },
   {
    "duration": 520,
    "start_time": "2023-09-13T17:05:53.481Z"
   },
   {
    "duration": 523,
    "start_time": "2023-09-13T17:06:00.163Z"
   },
   {
    "duration": 1597,
    "start_time": "2023-09-13T17:08:11.652Z"
   },
   {
    "duration": 1168,
    "start_time": "2023-09-13T17:08:13.251Z"
   },
   {
    "duration": 1722,
    "start_time": "2023-09-13T17:08:14.421Z"
   },
   {
    "duration": 71,
    "start_time": "2023-09-13T17:08:16.146Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:08:16.218Z"
   },
   {
    "duration": 11,
    "start_time": "2023-09-13T17:08:16.244Z"
   },
   {
    "duration": 22,
    "start_time": "2023-09-13T17:08:16.256Z"
   },
   {
    "duration": 346,
    "start_time": "2023-09-13T17:08:16.280Z"
   },
   {
    "duration": 13881,
    "start_time": "2023-09-13T17:08:16.628Z"
   },
   {
    "duration": 6463,
    "start_time": "2023-09-13T17:08:30.511Z"
   },
   {
    "duration": 33475,
    "start_time": "2023-09-13T17:08:36.976Z"
   },
   {
    "duration": 111205,
    "start_time": "2023-09-13T17:09:10.453Z"
   },
   {
    "duration": 2086,
    "start_time": "2023-09-13T17:11:01.659Z"
   },
   {
    "duration": 550,
    "start_time": "2023-09-13T17:11:03.747Z"
   },
   {
    "duration": 329,
    "start_time": "2023-09-13T17:11:04.299Z"
   },
   {
    "duration": 501,
    "start_time": "2023-09-13T17:11:23.016Z"
   },
   {
    "duration": 1541,
    "start_time": "2023-09-13T17:15:33.090Z"
   },
   {
    "duration": 1188,
    "start_time": "2023-09-13T17:15:34.633Z"
   },
   {
    "duration": 1677,
    "start_time": "2023-09-13T17:15:35.823Z"
   },
   {
    "duration": 73,
    "start_time": "2023-09-13T17:15:37.502Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:15:37.577Z"
   },
   {
    "duration": 27,
    "start_time": "2023-09-13T17:15:37.582Z"
   },
   {
    "duration": 88,
    "start_time": "2023-09-13T17:15:37.611Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.700Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.701Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.702Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.704Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.704Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.706Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.707Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:15:37.708Z"
   },
   {
    "duration": 13,
    "start_time": "2023-09-13T17:16:59.496Z"
   },
   {
    "duration": 1587,
    "start_time": "2023-09-13T17:17:07.538Z"
   },
   {
    "duration": 1163,
    "start_time": "2023-09-13T17:17:09.127Z"
   },
   {
    "duration": 1734,
    "start_time": "2023-09-13T17:17:10.291Z"
   },
   {
    "duration": 76,
    "start_time": "2023-09-13T17:17:12.027Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:17:12.104Z"
   },
   {
    "duration": 16,
    "start_time": "2023-09-13T17:17:12.109Z"
   },
   {
    "duration": 27,
    "start_time": "2023-09-13T17:17:12.126Z"
   },
   {
    "duration": 356,
    "start_time": "2023-09-13T17:17:12.155Z"
   },
   {
    "duration": 14269,
    "start_time": "2023-09-13T17:17:12.513Z"
   },
   {
    "duration": 6398,
    "start_time": "2023-09-13T17:17:26.784Z"
   },
   {
    "duration": 33964,
    "start_time": "2023-09-13T17:17:33.184Z"
   },
   {
    "duration": 104614,
    "start_time": "2023-09-13T17:18:07.149Z"
   },
   {
    "duration": 2093,
    "start_time": "2023-09-13T17:19:51.765Z"
   },
   {
    "duration": 512,
    "start_time": "2023-09-13T17:19:53.860Z"
   },
   {
    "duration": 321,
    "start_time": "2023-09-13T17:19:54.374Z"
   },
   {
    "duration": 1640,
    "start_time": "2023-09-13T17:34:01.711Z"
   },
   {
    "duration": 1275,
    "start_time": "2023-09-13T17:34:03.353Z"
   },
   {
    "duration": 1718,
    "start_time": "2023-09-13T17:34:04.630Z"
   },
   {
    "duration": 94,
    "start_time": "2023-09-13T17:34:06.349Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:34:06.446Z"
   },
   {
    "duration": 27,
    "start_time": "2023-09-13T17:34:06.451Z"
   },
   {
    "duration": 28,
    "start_time": "2023-09-13T17:34:06.480Z"
   },
   {
    "duration": 345,
    "start_time": "2023-09-13T17:34:06.510Z"
   },
   {
    "duration": 84,
    "start_time": "2023-09-13T17:34:06.857Z"
   },
   {
    "duration": 6281,
    "start_time": "2023-09-13T17:34:06.943Z"
   },
   {
    "duration": 34181,
    "start_time": "2023-09-13T17:34:13.226Z"
   },
   {
    "duration": 1675,
    "start_time": "2023-09-13T17:35:02.419Z"
   },
   {
    "duration": 1216,
    "start_time": "2023-09-13T17:35:04.096Z"
   },
   {
    "duration": 2046,
    "start_time": "2023-09-13T17:35:05.314Z"
   },
   {
    "duration": 82,
    "start_time": "2023-09-13T17:35:07.362Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:35:07.447Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-13T17:35:07.453Z"
   },
   {
    "duration": 19,
    "start_time": "2023-09-13T17:35:07.478Z"
   },
   {
    "duration": 379,
    "start_time": "2023-09-13T17:35:07.498Z"
   },
   {
    "duration": 245,
    "start_time": "2023-09-13T17:35:07.879Z"
   },
   {
    "duration": 6299,
    "start_time": "2023-09-13T17:35:08.126Z"
   },
   {
    "duration": 33664,
    "start_time": "2023-09-13T17:35:14.427Z"
   },
   {
    "duration": 1603,
    "start_time": "2023-09-13T17:36:23.693Z"
   },
   {
    "duration": 1165,
    "start_time": "2023-09-13T17:36:25.298Z"
   },
   {
    "duration": 1750,
    "start_time": "2023-09-13T17:36:26.464Z"
   },
   {
    "duration": 74,
    "start_time": "2023-09-13T17:36:28.217Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T17:36:28.292Z"
   },
   {
    "duration": 13,
    "start_time": "2023-09-13T17:36:28.297Z"
   },
   {
    "duration": 36,
    "start_time": "2023-09-13T17:36:28.311Z"
   },
   {
    "duration": 366,
    "start_time": "2023-09-13T17:36:28.349Z"
   },
   {
    "duration": 195,
    "start_time": "2023-09-13T17:36:28.717Z"
   },
   {
    "duration": 6629,
    "start_time": "2023-09-13T17:36:28.914Z"
   },
   {
    "duration": 10982,
    "start_time": "2023-09-13T17:36:35.545Z"
   },
   {
    "duration": 1569,
    "start_time": "2023-09-13T17:37:26.778Z"
   },
   {
    "duration": 1154,
    "start_time": "2023-09-13T17:37:28.349Z"
   },
   {
    "duration": 1739,
    "start_time": "2023-09-13T17:37:29.504Z"
   },
   {
    "duration": 74,
    "start_time": "2023-09-13T17:37:31.245Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-13T17:37:31.321Z"
   },
   {
    "duration": 39,
    "start_time": "2023-09-13T17:37:31.346Z"
   },
   {
    "duration": 30,
    "start_time": "2023-09-13T17:37:31.387Z"
   },
   {
    "duration": 366,
    "start_time": "2023-09-13T17:37:31.419Z"
   },
   {
    "duration": 139,
    "start_time": "2023-09-13T17:37:31.786Z"
   },
   {
    "duration": 6353,
    "start_time": "2023-09-13T17:37:31.928Z"
   },
   {
    "duration": 33615,
    "start_time": "2023-09-13T17:37:38.283Z"
   },
   {
    "duration": 109165,
    "start_time": "2023-09-13T17:38:11.899Z"
   },
   {
    "duration": 2122,
    "start_time": "2023-09-13T17:40:01.066Z"
   },
   {
    "duration": 496,
    "start_time": "2023-09-13T17:40:03.190Z"
   },
   {
    "duration": 307,
    "start_time": "2023-09-13T17:40:03.688Z"
   },
   {
    "duration": 1586,
    "start_time": "2023-09-13T17:40:24.674Z"
   },
   {
    "duration": 1171,
    "start_time": "2023-09-13T17:40:26.262Z"
   },
   {
    "duration": 38016,
    "start_time": "2023-09-13T17:40:27.434Z"
   },
   {
    "duration": 10051,
    "start_time": "2023-09-13T17:41:05.452Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:41:15.505Z"
   },
   {
    "duration": 83,
    "start_time": "2023-09-13T17:41:15.510Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-13T17:41:15.595Z"
   },
   {
    "duration": 706,
    "start_time": "2023-09-13T17:41:15.620Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:41:16.328Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:41:16.329Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:41:16.330Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:41:16.331Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:41:16.332Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:41:16.333Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-13T17:41:16.334Z"
   },
   {
    "duration": 1726,
    "start_time": "2023-09-13T17:42:24.259Z"
   },
   {
    "duration": 1220,
    "start_time": "2023-09-13T17:42:25.988Z"
   },
   {
    "duration": 40759,
    "start_time": "2023-09-13T17:42:27.210Z"
   },
   {
    "duration": 10396,
    "start_time": "2023-09-13T17:43:07.970Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T17:43:18.369Z"
   },
   {
    "duration": 54,
    "start_time": "2023-09-13T17:43:18.374Z"
   },
   {
    "duration": 17,
    "start_time": "2023-09-13T17:43:18.430Z"
   },
   {
    "duration": 1645,
    "start_time": "2023-09-13T17:43:18.448Z"
   },
   {
    "duration": 72185,
    "start_time": "2023-09-13T17:43:20.094Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:44:32.282Z"
   },
   {
    "duration": 1692,
    "start_time": "2023-09-13T17:44:53.158Z"
   },
   {
    "duration": 1211,
    "start_time": "2023-09-13T17:44:54.856Z"
   },
   {
    "duration": 1792,
    "start_time": "2023-09-13T17:44:56.068Z"
   },
   {
    "duration": 92,
    "start_time": "2023-09-13T17:44:57.863Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T17:44:57.957Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-13T17:44:57.963Z"
   },
   {
    "duration": 35,
    "start_time": "2023-09-13T17:44:57.987Z"
   },
   {
    "duration": 396,
    "start_time": "2023-09-13T17:44:58.024Z"
   },
   {
    "duration": 1650,
    "start_time": "2023-09-13T17:45:10.735Z"
   },
   {
    "duration": 1243,
    "start_time": "2023-09-13T17:45:12.386Z"
   },
   {
    "duration": 2063,
    "start_time": "2023-09-13T17:45:13.630Z"
   },
   {
    "duration": 84,
    "start_time": "2023-09-13T17:45:15.695Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T17:45:15.781Z"
   },
   {
    "duration": 41,
    "start_time": "2023-09-13T17:45:15.786Z"
   },
   {
    "duration": 60,
    "start_time": "2023-09-13T17:45:15.829Z"
   },
   {
    "duration": 423,
    "start_time": "2023-09-13T17:45:15.890Z"
   },
   {
    "duration": 14599,
    "start_time": "2023-09-13T17:45:16.314Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T17:45:30.915Z"
   },
   {
    "duration": 6763,
    "start_time": "2023-09-13T17:45:30.920Z"
   },
   {
    "duration": 35424,
    "start_time": "2023-09-13T17:45:37.685Z"
   },
   {
    "duration": 1728,
    "start_time": "2023-09-13T17:55:13.473Z"
   },
   {
    "duration": 1223,
    "start_time": "2023-09-13T17:55:15.203Z"
   },
   {
    "duration": 1736,
    "start_time": "2023-09-13T17:55:16.428Z"
   },
   {
    "duration": 24,
    "start_time": "2023-09-13T17:55:18.166Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T17:55:18.192Z"
   },
   {
    "duration": 33,
    "start_time": "2023-09-13T17:55:18.196Z"
   },
   {
    "duration": 26,
    "start_time": "2023-09-13T17:55:18.231Z"
   },
   {
    "duration": 378,
    "start_time": "2023-09-13T17:55:18.259Z"
   },
   {
    "duration": 3451,
    "start_time": "2023-09-13T17:55:18.648Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-13T17:55:22.100Z"
   },
   {
    "duration": 1420,
    "start_time": "2023-09-13T17:55:22.105Z"
   },
   {
    "duration": 27460,
    "start_time": "2023-09-13T17:55:23.526Z"
   },
   {
    "duration": 388771,
    "start_time": "2023-09-13T17:55:50.988Z"
   },
   {
    "duration": 1752,
    "start_time": "2023-09-13T18:02:19.762Z"
   },
   {
    "duration": 502,
    "start_time": "2023-09-13T18:02:21.516Z"
   },
   {
    "duration": 291,
    "start_time": "2023-09-13T18:02:22.020Z"
   },
   {
    "duration": 1571,
    "start_time": "2023-09-13T18:02:49.362Z"
   },
   {
    "duration": 1180,
    "start_time": "2023-09-13T18:02:50.935Z"
   },
   {
    "duration": 41907,
    "start_time": "2023-09-13T18:02:52.116Z"
   },
   {
    "duration": 11700,
    "start_time": "2023-09-13T18:03:34.025Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T18:03:45.728Z"
   },
   {
    "duration": 72,
    "start_time": "2023-09-13T18:03:45.743Z"
   },
   {
    "duration": 13,
    "start_time": "2023-09-13T18:03:45.818Z"
   },
   {
    "duration": 1778,
    "start_time": "2023-09-13T18:03:45.833Z"
   },
   {
    "duration": 2961,
    "start_time": "2023-09-13T18:41:32.560Z"
   },
   {
    "duration": 1283,
    "start_time": "2023-09-13T18:41:35.523Z"
   },
   {
    "duration": 44424,
    "start_time": "2023-09-13T18:41:36.807Z"
   },
   {
    "duration": 11643,
    "start_time": "2023-09-13T18:42:21.233Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-13T18:42:32.878Z"
   },
   {
    "duration": 54,
    "start_time": "2023-09-13T18:42:32.884Z"
   },
   {
    "duration": 22,
    "start_time": "2023-09-13T18:42:32.943Z"
   },
   {
    "duration": 2223,
    "start_time": "2023-09-13T18:42:32.967Z"
   },
   {
    "duration": 4531,
    "start_time": "2023-09-14T16:27:44.697Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.233Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.234Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.235Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.237Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.239Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.241Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.242Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.243Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.244Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.246Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.249Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.250Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.252Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.254Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.255Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:27:49.257Z"
   },
   {
    "duration": 11004,
    "start_time": "2023-09-14T16:28:14.080Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.087Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.088Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.089Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.091Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.091Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.093Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.094Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.110Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.112Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.113Z"
   },
   {
    "duration": 1,
    "start_time": "2023-09-14T16:28:25.114Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.116Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.118Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.119Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.120Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:28:25.121Z"
   },
   {
    "duration": 4711,
    "start_time": "2023-09-14T16:28:32.171Z"
   },
   {
    "duration": 2726,
    "start_time": "2023-09-14T16:28:36.884Z"
   },
   {
    "duration": 1544,
    "start_time": "2023-09-14T16:28:39.612Z"
   },
   {
    "duration": 27,
    "start_time": "2023-09-14T16:28:41.159Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-14T16:28:41.187Z"
   },
   {
    "duration": 123,
    "start_time": "2023-09-14T16:28:41.193Z"
   },
   {
    "duration": 135,
    "start_time": "2023-09-14T16:28:41.317Z"
   },
   {
    "duration": 66,
    "start_time": "2023-09-14T16:28:41.454Z"
   },
   {
    "duration": 360,
    "start_time": "2023-09-14T16:28:41.521Z"
   },
   {
    "duration": 3762,
    "start_time": "2023-09-14T16:28:41.883Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-14T16:28:45.646Z"
   },
   {
    "duration": 1598,
    "start_time": "2023-09-14T16:28:45.651Z"
   },
   {
    "duration": 4256,
    "start_time": "2023-09-14T16:29:04.122Z"
   },
   {
    "duration": 1239,
    "start_time": "2023-09-14T16:29:08.381Z"
   },
   {
    "duration": 1347,
    "start_time": "2023-09-14T16:29:09.622Z"
   },
   {
    "duration": 45,
    "start_time": "2023-09-14T16:29:10.971Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-14T16:29:11.018Z"
   },
   {
    "duration": 11,
    "start_time": "2023-09-14T16:29:11.023Z"
   },
   {
    "duration": 9,
    "start_time": "2023-09-14T16:29:11.035Z"
   },
   {
    "duration": 14,
    "start_time": "2023-09-14T16:29:11.046Z"
   },
   {
    "duration": 396,
    "start_time": "2023-09-14T16:29:11.062Z"
   },
   {
    "duration": 3808,
    "start_time": "2023-09-14T16:29:11.460Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-14T16:29:15.270Z"
   },
   {
    "duration": 1499,
    "start_time": "2023-09-14T16:29:15.275Z"
   },
   {
    "duration": 9980,
    "start_time": "2023-09-14T16:29:16.776Z"
   },
   {
    "duration": 37370,
    "start_time": "2023-09-14T16:29:26.757Z"
   },
   {
    "duration": 2032,
    "start_time": "2023-09-14T16:30:04.129Z"
   },
   {
    "duration": 566,
    "start_time": "2023-09-14T16:30:06.163Z"
   },
   {
    "duration": 579,
    "start_time": "2023-09-14T16:30:06.731Z"
   },
   {
    "duration": 4773,
    "start_time": "2023-09-14T16:30:59.278Z"
   },
   {
    "duration": 1277,
    "start_time": "2023-09-14T16:31:04.054Z"
   },
   {
    "duration": 2311,
    "start_time": "2023-09-14T16:31:05.334Z"
   },
   {
    "duration": 24,
    "start_time": "2023-09-14T16:31:07.647Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-14T16:31:07.673Z"
   },
   {
    "duration": 40,
    "start_time": "2023-09-14T16:31:07.678Z"
   },
   {
    "duration": 8,
    "start_time": "2023-09-14T16:31:07.720Z"
   },
   {
    "duration": 14,
    "start_time": "2023-09-14T16:31:07.730Z"
   },
   {
    "duration": 405,
    "start_time": "2023-09-14T16:31:07.746Z"
   },
   {
    "duration": 3591,
    "start_time": "2023-09-14T16:31:08.154Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-14T16:31:11.747Z"
   },
   {
    "duration": 1545,
    "start_time": "2023-09-14T16:31:11.753Z"
   },
   {
    "duration": 28067,
    "start_time": "2023-09-14T16:31:13.300Z"
   },
   {
    "duration": 32755,
    "start_time": "2023-09-14T16:31:41.369Z"
   },
   {
    "duration": 2021,
    "start_time": "2023-09-14T16:32:14.126Z"
   },
   {
    "duration": 554,
    "start_time": "2023-09-14T16:32:16.149Z"
   },
   {
    "duration": 584,
    "start_time": "2023-09-14T16:32:16.705Z"
   },
   {
    "duration": 4336,
    "start_time": "2023-09-14T16:35:40.332Z"
   },
   {
    "duration": 1264,
    "start_time": "2023-09-14T16:35:44.670Z"
   },
   {
    "duration": 12706,
    "start_time": "2023-09-14T16:35:45.936Z"
   },
   {
    "duration": 1480,
    "start_time": "2023-09-14T16:35:58.644Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.126Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.128Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.129Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.131Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.132Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.134Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.136Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.137Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.139Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.140Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.142Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.143Z"
   },
   {
    "duration": 0,
    "start_time": "2023-09-14T16:36:00.144Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.383px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
